# èŠå¤©è®°å½•åˆ†æå®Œæ•´æ–¹æ¡ˆ

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-02-05  
**åŸºäº**: ä¸‰ä»½èŠå¤©åˆ†ææ–‡æ¡£ç»¼åˆç ”ç©¶ + 2024-2025æœ€æ–°æŠ€æœ¯è°ƒç ”

---

## ç›®å½•

1. [æ–¹æ¡ˆæ¦‚è¿°](#1-æ–¹æ¡ˆæ¦‚è¿°)
2. [åˆ†æç»´åº¦ä½“ç³»](#2-åˆ†æç»´åº¦ä½“ç³»)
3. [æŠ€æœ¯æ¶æ„è®¾è®¡](#3-æŠ€æœ¯æ¶æ„è®¾è®¡)
4. [æ ¸å¿ƒç®—æ³•å®ç°](#4-æ ¸å¿ƒç®—æ³•å®ç°)
5. [æ€§æ ¼åˆ†ææ–¹æ³•è®º](#5-æ€§æ ¼åˆ†ææ–¹æ³•è®º)
6. [ç¤¾ä¼šå·¥ç¨‹å­¦ä¿¡æ¯æå–](#6-ç¤¾ä¼šå·¥ç¨‹å­¦ä¿¡æ¯æå–)
7. [éšç§ä¿æŠ¤ä¸ä¼¦ç†åˆè§„](#7-éšç§ä¿æŠ¤ä¸ä¼¦ç†åˆè§„)
8. [å·¥å…·ä¸æŠ€æœ¯æ ˆ](#8-å·¥å…·ä¸æŠ€æœ¯æ ˆ)
9. [å®æ–½è·¯çº¿å›¾](#9-å®æ–½è·¯çº¿å›¾)
10. [é™„å½•](#10-é™„å½•)

---

## 1. æ–¹æ¡ˆæ¦‚è¿°

### 1.1 èƒŒæ™¯ä¸ç›®æ ‡

èŠå¤©è®°å½•åˆ†ææ˜¯ä»æµ·é‡é€šè®¯æ•°æ®ä¸­æå–æ·±å±‚æ´å¯Ÿçš„ç³»ç»Ÿæ€§å·¥ç¨‹ã€‚éšç€å³æ—¶é€šè®¯å·¥å…·çš„æ™®åŠï¼ŒèŠå¤©è®°å½•å·²æˆä¸ºç ”ç©¶äººç±»äº¤äº’æ¨¡å¼ã€ç†è§£ç”¨æˆ·è¡Œä¸ºå’Œæ£€æµ‹å®‰å…¨å¨èƒçš„é‡è¦æ•°æ®æºã€‚æœ¬æ–¹æ¡ˆæ—¨åœ¨æä¾›ä¸€å¥—å®Œæ•´ã€å¯æ“ä½œçš„èŠå¤©è®°å½•åˆ†ææ¡†æ¶ï¼Œæ¶µç›–ä»æ•°æ®é‡‡é›†åˆ°æ´å¯Ÿè¾“å‡ºçš„å…¨æµç¨‹ã€‚

### 1.2 æ–¹æ¡ˆä»·å€¼

| ä»·å€¼ç»´åº¦ | åº”ç”¨åœºæ™¯ |
|---------|---------|
| **ç”¨æˆ·æ´å¯Ÿ** | å®¢æˆ·ç”»åƒã€è¡Œä¸ºæ¨¡å¼ã€æ¶ˆè´¹åå¥½ |
| **å¿ƒç†å¥åº·** | æƒ…æ„Ÿç›‘æµ‹ã€å±æœºé¢„è­¦ã€å¿ƒç†è¾…å¯¼ |
| **å®‰å…¨ç®¡ç†** | å¨èƒæ£€æµ‹ã€å¼‚å¸¸è¡Œä¸ºã€å†…éƒ¨å®¡è®¡ |
| **å•†ä¸šæ™ºèƒ½** | å¸‚åœºåˆ†æã€ç«å“ç ”ç©¶ã€è¶‹åŠ¿é¢„æµ‹ |
| **å­¦æœ¯ç ”ç©¶** | è¯­è¨€å­¦ã€å¿ƒç†å­¦ã€ç¤¾ä¼šå­¦ç ”ç©¶ |

### 1.3 æ ¸å¿ƒæŒ‘æˆ˜

1. **æ•°æ®ç¢ç‰‡åŒ–**ï¼šèŠå¤©è®°å½•å…·æœ‰çŸ­æ–‡æœ¬ã€å£è¯­åŒ–ã€è¯­å¢ƒä¾èµ–ç­‰ç‰¹ç‚¹
2. **éšç§æ•æ„Ÿæ€§**ï¼šæ¶‰åŠä¸ªäººé€šè®¯å†…å®¹ï¼Œæ•°æ®å¤„ç†éœ€ä¸¥æ ¼åˆè§„
3. **è¯­ä¹‰å¤æ‚æ€§**ï¼šä¸­æ–‡è¯­ä¹‰ç†è§£éœ€è¦ä¸“ä¸šçš„NLPæŠ€æœ¯æ”¯æŒ
4. **å¤šæ¨¡æ€æŒ‘æˆ˜**ï¼šé™¤æ–‡æœ¬å¤–ï¼Œè¿˜åŒ…æ‹¬è¡¨æƒ…ã€å›¾ç‰‡ã€è¯­éŸ³ç­‰å¤šç§å½¢å¼
5. **å®æ—¶æ€§è¦æ±‚**ï¼šéƒ¨åˆ†åœºæ™¯éœ€è¦å®æ—¶åˆ†æå’Œå¿«é€Ÿå“åº”

### 1.4 æ–¹æ¡ˆæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    èŠå¤©è®°å½•åˆ†æç³»ç»Ÿæ¶æ„                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  æ•°æ®é‡‡é›†å±‚  â”‚  â”‚  æ•°æ®å­˜å‚¨å±‚  â”‚  â”‚  æ•°æ®å¤„ç†å±‚  â”‚         â”‚
â”‚  â”‚  - APIæ¥å…¥  â”‚  â”‚  - å…³ç³»å‹   â”‚  â”‚  - æ¸…æ´—     â”‚         â”‚
â”‚  â”‚  - æ–‡ä»¶è§£æ â”‚  â”‚  - æ—¶åºæ•°æ®åº“â”‚  â”‚  - æ ‡å‡†åŒ–   â”‚         â”‚
â”‚  â”‚  - çˆ¬è™«é‡‡é›† â”‚  â”‚  - å›¾æ•°æ®åº“ â”‚  â”‚  - ç‰¹å¾åŒ–   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚          â”‚               â”‚               â”‚                 â”‚
â”‚          â–¼               â–¼               â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                   åˆ†æå¼•æ“å±‚                      â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚
â”‚  â”‚  â”‚æ—¶é—´åˆ†æ â”‚ â”‚å†…å®¹åˆ†æ â”‚ â”‚æƒ…æ„Ÿåˆ†æ â”‚ â”‚ç¤¾äº¤åˆ†æ â”‚ â”‚       â”‚
â”‚  â”‚  â”‚æ¨¡å—    â”‚ â”‚æ¨¡å—    â”‚ â”‚æ¨¡å—    â”‚ â”‚æ¨¡å—    â”‚ â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚          â”‚               â”‚               â”‚                 â”‚
â”‚          â–¼               â–¼               â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                   åº”ç”¨è¾“å‡ºå±‚                      â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚
â”‚  â”‚  â”‚å¯è§†åŒ–  â”‚ â”‚æŠ¥å‘Šç”Ÿæˆ â”‚ â”‚å‘Šè­¦æ¨é€ â”‚ â”‚APIæœåŠ¡  â”‚ â”‚       â”‚
â”‚  â”‚  â”‚Dashboardâ”‚ â”‚PDF/HTML â”‚ â”‚å®æ—¶é€šçŸ¥ â”‚ â”‚Restful  â”‚ â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. åˆ†æç»´åº¦ä½“ç³»

### 2.1 æ—¶é—´ç»´åº¦åˆ†æ

æ—¶é—´ç»´åº¦æ˜¯èŠå¤©è®°å½•ä¸­æœ€å®¢è§‚çš„å…ƒæ•°æ®ï¼Œè•´å«ä¸°å¯Œçš„è¡Œä¸ºè§„å¾‹ä¿¡æ¯ã€‚

#### 2.1.1 æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | è®¡ç®—æ–¹æ³• | ä¸šåŠ¡å«ä¹‰ |
|---------|---------|---------|
| **æ´»è·ƒåº¦** | æ¶ˆæ¯æ•°/æ—¶é—´æ®µ | ç”¨æˆ·å‚ä¸ç¨‹åº¦ |
| **å“åº”å»¶è¿Ÿ** | ç›¸é‚»æ¶ˆæ¯æ—¶é—´å·® | æ²Ÿé€šæ•ˆç‡ |
| **åœ¨çº¿æ—¶é•¿** | ç´¯è®¡åœ¨çº¿æ—¶é—´ | æŠ•å…¥ç¨‹åº¦ |
| **æ—¶é—´ç†µ** | é¦™å†œç†µå…¬å¼ H=-Î£p*log(p) | è¡Œä¸ºè§„å¾‹æ€§ |
| **å‘¨æœŸç³»æ•°** | FFTå‘¨æœŸåˆ†æ | å‘¨æœŸæ€§æ¨¡å¼ |

#### 2.1.2 åˆ†ææ–¹æ³•

**è¡Œä¸ºè„‰æå»ºæ¨¡**ï¼š
```
1. æŒ‰å°æ—¶èšåˆæ¶ˆæ¯åˆ†å¸ƒ
2. è®¡ç®—å„æ—¶æ®µçš„æ¶ˆæ¯å¯†åº¦
3. è¯†åˆ«æ´»è·ƒæ—¶é—´æ®µï¼ˆPeak Hoursï¼‰
4. åˆ†æå·¥ä½œæ—¥vså‘¨æœ«å·®å¼‚
5. æ£€æµ‹å¼‚å¸¸æ—¶é—´æ´»åŠ¨
```

**æ—¶é—´ç†µè®¡ç®—**ï¼š
```python
import numpy as np
from scipy.stats import entropy

def calculate_time_entropy(messages, bins=24):
    """è®¡ç®—æ—¶é—´ç†µï¼Œåæ˜ è¡Œä¸ºè§„å¾‹æ€§"""
    hours = [msg.timestamp.hour for msg in messages]
    hist, _ = np.histogram(hours, bins=bins, range=(0, 24))
    prob = hist / hist.sum()
    return entropy(prob)
```

**é©¬å°”å¯å¤«é“¾åˆ†æ**ï¼š
```python
class ActivityMarkovChain:
    def __init__(self):
        self.transition_matrix = {}
    
    def fit(self, activity_sequence):
        """æ„å»ºæ´»åŠ¨è½¬ç§»çŸ©é˜µ"""
        for i in range(len(activity_sequence) - 1):
            current = activity_sequence[i]
            next_state = activity_sequence[i + 1]
            if current not in self.transition_matrix:
                self.transition_matrix[current] = Counter()
            self.transition_matrix[current][next_state] += 1
        # è½¬æ¢ä¸ºæ¦‚ç‡çŸ©é˜µ
        self._normalize_matrix()
    
    def predict_next_activity(self, current_activity):
        """é¢„æµ‹ä¸‹ä¸€ä¸ªæ´»åŠ¨çŠ¶æ€"""
        if current_activity in self.transition_matrix:
            next_states = self.transition_matrix[current_activity]
            return max(next_states, key=next_states.get)
        return None
```

#### 2.1.3 åº”ç”¨åœºæ™¯

- ç”¨æˆ·ä½œæ¯è§„å¾‹ç›‘æµ‹
- å‘˜å·¥å·¥ä½œçŠ¶æ€è¯„ä¼°
- å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ï¼ˆå¦‚æ·±å¤œæ´»è·ƒï¼‰
- ç¤¾äº¤èŠ‚å¥åˆ†æ

### 2.2 å†…å®¹ç»´åº¦åˆ†æ

å†…å®¹ç»´åº¦å…³æ³¨èŠå¤©æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯å’Œä¸»é¢˜ç»“æ„ã€‚

#### 2.2.1 ä¸»é¢˜åˆ†æ

**LDAä¸»é¢˜æ¨¡å‹**ï¼š
```python
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

class TopicAnalyzer:
    def __init__(self, n_topics=10):
        self.n_topics = n_topics
        self.vectorizer = CountVectorizer(max_df=0.95, min_df=2)
        self.lda = LatentDirichletAllocation(
            n_components=n_topics,
            max_iter=20,
            learning_method='online'
        )
    
    def extract_topics(self, documents):
        """æå–ä¸»é¢˜åˆ†å¸ƒ"""
        doc_term_matrix = self.vectorizer.fit_transform(documents)
        topic_distributions = self.lda.fit_transform(doc_term_matrix)
        
        feature_names = self.vectorizer.get_feature_names_out()
        topics = []
        for topic_idx, topic in enumerate(self.lda.components_):
            top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]
            topics.append({
                'topic_id': topic_idx,
                'words': top_words,
                'weight': topic_distributions[:, topic_idx].mean()
            })
        return topics
```

**BERTopicåŠ¨æ€ä¸»é¢˜å»ºæ¨¡**ï¼ˆ2024å¹´æœ€æ–°æ–¹æ³•ï¼‰ï¼š
```python
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

class DynamicTopicAnalyzer:
    def __init__(self):
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.topic_model = BERTopic(
            embedding_model=self.embedding_model,
            language='multilingual',
            nr_topics='auto'
        )
    
    def analyze_evolution(self, documents, timestamps):
        """åˆ†æä¸»é¢˜éšæ—¶é—´çš„æ¼”å˜"""
        # æŒ‰æ—¶é—´åˆ‡ç‰‡
        time_slices = self._create_time_slices(timestamps)
        # æå–æ¯ä¸ªæ—¶é—´æ®µçš„ä¸»é¢˜
        topic_evolution = []
        for slice_docs, slice_time in zip(time_slices, timestamps):
            topics, _ = self.topic_model.fit_transform(slice_docs)
            topic_evolution.append({
                'time': slice_time,
                'topics': topics
            })
        return topic_evolution
```

#### 2.2.2 å…³é”®è¯æå–

| ç®—æ³• | åŸç† | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | å±€é™ |
|-----|------|---------|------|------|
| **TF-IDF** | è¯é¢‘Ã—é€†æ–‡æ¡£é¢‘ç‡ | å¤§è§„æ¨¡è¯­æ–™ | ç®€å•é«˜æ•ˆ | éš¾å¤„ç†åŒä¹‰è¯ |
| **TextRank** | å›¾è®ºè¿­ä»£æŠ•ç¥¨ | å•æ–‡æ¡£æ‘˜è¦ | æ— ç›‘ç£ | è®¡ç®—å¼€é”€å¤§ |
| **YAKE** | å•æ–‡æ¡£ç»Ÿè®¡ç‰¹å¾ | å³æ—¶é€šè®¯ | æ— è¯­æ–™åº“ä¾èµ– | å—æç«¯è¯é¢‘å¹²æ‰° |
| **KeyBERT** | è¯­ä¹‰å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦ | é«˜ç²¾åº¦åœºæ™¯ | è¯­ä¹‰ç†è§£å¼º | ä¾èµ–GPU |
| **SIFRank** | è¯æ€§+ELMoåµŒå…¥ | ä¸­æ–‡åœºæ™¯ | ä¸­æ–‡äººæ•ˆé«˜ | æ¨¡å‹å¤æ‚ |

**ç»¼åˆå…³é”®è¯æå–å™¨**ï¼š
```python
class KeywordExtractor:
    def __init__(self):
        self.tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.textrank = TextRank()
        self.bert_model = None  # å¯é€‰ï¼šåŠ è½½BERTæ¨¡å‹
    
    def extract_keywords(self, documents, method='ensemble'):
        """ç»¼åˆå…³é”®è¯æå–"""
        if method == 'tfidf':
            return self._extract_tfidf(documents)
        elif method == 'textrank':
            return self._extract_textrank(documents)
        elif method == 'ensemble':
            return self._ensemble_extraction(documents)
    
    def _ensemble_extraction(self, documents):
        """é›†æˆæå–æ–¹æ³•"""
        tfidf_results = self._extract_tfidf(documents)
        textrank_results = self._extract_textrank(documents)
        
        # èåˆä¸¤ç§æ–¹æ³•çš„ç»“æœ
        combined_scores = {}
        for word, score in tfidf_results:
            combined_scores[word] = combined_scores.get(word, 0) + score * 0.6
        for word, score in textrank_results:
            combined_scores[word] = combined_scores.get(word, 0) + score * 0.4
        
        # è¿”å›æ’åºç»“æœ
        sorted_keywords = sorted(
            combined_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:50]
        return sorted_keywords
```

#### 2.2.3 è¯­ä¹‰æ·±åº¦åˆ†æ

**BERTè¯­ä¹‰ç›¸ä¼¼åº¦**ï¼š
```python
from transformers import BertModel, BertTokenizer
import torch

class SemanticAnalyzer:
    def __init__(self, model_name='bert-base-chinese'):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertModel.from_pretrained(model_name)
        self.model.eval()
    
    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„è¯­ä¹‰å‘é‡"""
        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :].squeeze().numpy()
    
    def calculate_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        vec1 = self.get_embedding(text1)
        vec2 = self.get_embedding(text2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

### 2.3 æƒ…æ„Ÿç»´åº¦åˆ†æ

æƒ…æ„Ÿåˆ†ææ˜¯ç†è§£ç”¨æˆ·å¿ƒç†çŠ¶æ€å’Œå¯¹è¯è´¨é‡çš„å…³é”®æŠ€æœ¯ã€‚

#### 2.3.1 æƒ…æ„Ÿåˆ†ææ¡†æ¶

```
æƒ…æ„Ÿåˆ†æå±‚æ¬¡ç»“æ„ï¼š
â”œâ”€â”€ ææ€§åˆ†æï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰
â”œâ”€â”€ æƒ…ç»ªåˆ†ç±»ï¼ˆå–œæ‚¦ã€æ„¤æ€’ã€æ‚²ä¼¤ã€ææƒ§ã€æƒŠè®¶ã€åŒæ¶ï¼‰
â”œâ”€â”€ æƒ…æ„Ÿå¼ºåº¦ï¼ˆ1-5åˆ†æˆ–0-1è¿ç»­å€¼ï¼‰
â””â”€â”€ æ–¹é¢çº§æƒ…æ„Ÿï¼ˆé’ˆå¯¹ç‰¹å®šè¯é¢˜çš„æƒ…æ„Ÿï¼‰
```

#### 2.3.2 ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹

**åŸºäºBERTçš„æƒ…æ„Ÿåˆ†æ**ï¼š
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

class ChineseSentimentAnalyzer:
    def __init__(self, model_name='uer/roberta-base-finetuned-chinanese-english'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()
    
    def analyze(self, text):
        """æƒ…æ„Ÿåˆ†æ"""
        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
        
        labels = ['negative', 'neutral', 'positive']
        result = {
            'sentiment': labels[probs.argmax().item()],
            'confidence': probs.max().item(),
            'scores': {
                'negative': probs[0][0].item(),
                'neutral': probs[0][1].item(),
                'positive': probs[0][2].item()
            }
        }
        return result
    
    def analyze_with_intensity(self, text):
        """å¸¦å¼ºåº¦çš„æƒ…æ„Ÿåˆ†æ"""
        base_result = self.analyze(text)
        # ç»“åˆæ ‡ç‚¹ã€è¡¨æƒ…ç­‰è®¡ç®—å¼ºåº¦
        exclamation_count = text.count('ï¼') + text.count('!')
        emoji_count = len(re.findall(r'[ğŸ˜€-ğŸ™]', text))
        
        intensity = min(1.0, (exclamation_count * 0.1 + emoji_count * 0.05 + 0.5))
        base_result['intensity'] = intensity
        
        return base_result
```

#### 2.3.3 æƒ…æ„Ÿè¶‹åŠ¿åˆ†æ

```python
import pandas as pd
from scipy.ndimage import gaussian_filter1d

class SentimentTrendAnalyzer:
    def __init__(self):
        self.sentiment_analyzer = ChineseSentimentAnalyzer()
    
    def analyze_trend(self, messages, window_size=10):
        """åˆ†ææƒ…æ„Ÿè¶‹åŠ¿"""
        sentiments = []
        for msg in messages:
            result = self.sentiment_analyzer.analyze(msg.content)
            sentiments.append({
                'timestamp': msg.timestamp,
                'sentiment': result['sentiment'],
                'score': result['scores']['positive'] - result['scores']['negative'],
                'confidence': result['confidence']
            })
        
        df = pd.DataFrame(sentiments)
        
        # å¹³æ»‘å¤„ç†
        df['smoothed_score'] = gaussian_filter1d(df['score'].values, sigma=3)
        
        # è¯†åˆ«æƒ…æ„Ÿè½¬æŠ˜ç‚¹
        inflection_points = self._detect_inflection_points(df)
        
        return {
            'trend_data': df,
            'inflection_points': inflection_points,
            'average_sentiment': df['score'].mean(),
            'sentiment_volatility': df['score'].std()
        }
    
    def _detect_inflection_points(self, df, threshold=0.3):
        """æ£€æµ‹æƒ…æ„Ÿè½¬æŠ˜ç‚¹"""
        inflection_points = []
        for i in range(1, len(df)):
            diff = df['smoothed_score'].iloc[i] - df['smoothed_score'].iloc[i-1]
            if abs(diff) > threshold:
                inflection_points.append({
                    'timestamp': df['timestamp'].iloc[i],
                    'direction': 'positive' if diff > 0 else 'negative',
                    'magnitude': abs(diff)
                })
        return inflection_points
```

#### 2.3.4 æƒ…æ„Ÿè¯æ±‡ç‰¹å¾

| æƒ…æ„Ÿç±»å‹ | è¯­è¨€ç‰¹å¾ | è¡Œä¸ºå…³è” |
|---------|---------|---------|
| **ç§¯ææƒ…ç»ª** | æ­£é¢å½¢å®¹è¯ã€é«˜é¢‘è¡¨æƒ…ç¬¦å·ã€æ„Ÿå¹å· | ç¤¾äº¤å‚ä¸åº¦é«˜ï¼Œå€¾å‘äºåˆä½œä¸æ¥çº³ |
| **æ¶ˆææƒ…ç»ª** | å¦å®šè¯å¢åŠ ã€è¯­æ°”ç”Ÿç¡¬ã€æ’ä»–æ€§è¯æ±‡ | æŒ«è´¥æ„Ÿã€é˜²å¾¡æ€§å¢å¼ºï¼Œå¯èƒ½å­˜åœ¨æµå¤±é£é™© |
| **ä¸­æ€§/å†·é™** | å¥å¼å®Œæ•´ã€é€»è¾‘è¿æ¥è¯ã€ä¸“ä¸šæœ¯è¯­ | ä»»åŠ¡å¯¼å‘ï¼Œæ³¨é‡äº‹å®äº¤æ¢ |
| **å¤æ‚æƒ…ç»ª** | è¯­æ„åè½¬ã€é•¿éš¾å¥ã€å¤šç§æƒ…æ„Ÿè¯æ··æ‚ | è®¤çŸ¥å†²çªã€é«˜åº¦æ•æ„Ÿ |

### 2.4 ç¤¾äº¤ç»´åº¦åˆ†æ

ç¤¾äº¤ç»´åº¦åˆ†ææ­ç¤ºç¾¤ä½“ç»“æ„ã€æƒåŠ›å…³ç³»å’Œä¿¡æ¯æµè½¬æ¨¡å¼ã€‚

#### 2.4.1 ç¤¾ä¼šç½‘ç»œåˆ†æ

```python
import networkx as nx
from collections import defaultdict

class SocialNetworkAnalyzer:
    def __init__(self):
        self.graph = nx.Graph()
    
    def build_network(self, messages):
        """æ„å»ºç¤¾äº¤ç½‘ç»œå›¾"""
        interaction_counts = defaultdict(lambda: defaultdict(int))
        
        for msg in messages:
            sender = msg.sender_id
            receiver = msg.receiver_id
            if sender and receiver:
                interaction_counts[sender][receiver] += 1
        
        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
        for sender, receivers in interaction_counts.items():
            self.graph.add_node(sender)
            for receiver, count in receivers.items():
                self.graph.add_node(receiver)
                self.graph.add_edge(sender, receiver, weight=count)
        
        return self.graph
    
    def calculate_centrality(self):
        """è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡"""
        return {
            'degree': nx.degree_centrality(self.graph),
            'betweenness': nx.betweenness_centrality(self.graph),
            'closeness': nx.closeness_centrality(self.graph),
            'pagerank': nx.pagerank(self.graph)
        }
    
    def identify_communities(self):
        """è¯†åˆ«ç¤¾åŒºç»“æ„"""
        from networkx.algorithms.community import louvain_communities
        communities = louvain_communities(self.graph)
        return [list(c) for c in communities]
    
    def analyze_power_dynamics(self):
        """æƒåŠ›åŠ¨æ€åˆ†æ - åŸºäºè¯­è¨€åè°ƒæ€§"""
        # æƒåŠ›è¾ƒä½è€…å€¾å‘äºæ¨¡ä»¿å¯¹æ–¹çš„åŠŸèƒ½è¯ä½¿ç”¨
        centrality = self.calculate_centrality()
        
        # è¯†åˆ«æ ¸å¿ƒæ„è§é¢†è¢–
        opinion_leaders = sorted(
            centrality['pagerank'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return {
            'core_members': opinion_leaders,
            'bridges': self._identify_bridges(),
            'isolates': list(nx.isolates(self.graph))
        }
    
    def _identify_bridges(self):
        """è¯†åˆ«æ¡¥æ¢èŠ‚ç‚¹"""
        bridges = []
        for node in self.graph.nodes():
            if nx.degree(self.graph, node) > 1:
                # è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§
                betweenness = nx.betweenness_centrality(self.graph)[node]
                if betweenness > 0.1:
                    bridges.append({
                        'node': node,
                        'betweenness': betweenness
                    })
        return sorted(bridges, key=lambda x: x['betweenness'], reverse=True)
```

#### 2.4.2 äº’åŠ¨æ¨¡å¼åˆ†æ

| æŒ‡æ ‡ | è®¡ç®—æ–¹æ³• | å«ä¹‰ |
|-----|---------|------|
| **äº’åŠ¨é¢‘ç‡** | æ¶ˆæ¯æ•°/æ—¶é—´æ®µ | ç¤¾äº¤æ´»è·ƒåº¦ |
| **å›å¤ç‡** | å›å¤æ¶ˆæ¯æ•°/æ€»æ¶ˆæ¯æ•° | å‚ä¸ç§¯ææ€§ |
| **é¦–æ¬¡å“åº”æ—¶é—´** | æ”¶åˆ°æ¶ˆæ¯åˆ°é¦–æ¬¡å›å¤çš„æ—¶é—´ | å“åº”é€Ÿåº¦ |
| **æ¶ˆæ¯é“¾æ¡é•¿åº¦** | è¿ç»­å¯¹è¯çš„è½®æ•° | æ·±å…¥ç¨‹åº¦ |
| **è¯é¢˜ä¸»å¯¼ç‡** | å‘èµ·è¯é¢˜çš„æ•°é‡/æ€»è¯é¢˜æ•° | é¢†å¯¼åŠ› |

#### 2.4.3 è¯­è¨€åè°ƒæ€§åˆ†æ

```python
class LinguisticCoordinationAnalyzer:
    def __init__(self):
        self.function_words = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±']
    
    def calculate_coordination(self, messages, person_a, person_b):
        """è®¡ç®—ä¸¤äººä¹‹é—´çš„è¯­è¨€åè°ƒæ€§"""
        words_a = self._extract_function_words([m for m in messages if m.sender_id == person_a])
        words_b = self._extract_function_words([m for m in messages if m.sender_id == person_b])
        
        # è®¡ç®—åŠŸèƒ½è¯ä½¿ç”¨é¢‘ç‡çš„ç›¸å…³æ€§
        freq_a = self._calculate_word_frequencies(words_a)
        freq_b = self._calculate_word_frequencies(words_b)
        
        correlation = self._pearson_correlation(freq_a, freq_b)
        
        # ä½åè°ƒæ€§å¯èƒ½æš—ç¤ºæƒåŠ›å·®å¼‚
        power_differential = self._infer_power_diff(correlation)
        
        return {
            'coordination_score': correlation,
            'power_differential': power_differential,
            'interpretation': self._interpret_coordination(correlation)
        }
    
    def _extract_function_words(self, messages):
        """æå–åŠŸèƒ½è¯"""
        all_words = []
        for msg in messages:
            words = jieba.lcut(msg.content)
            all_words.extend([w for w in words if w in self.function_words])
        return all_words
    
    def _infer_power_diff(self, coordination_score):
        """æ¨æ–­æƒåŠ›å·®å¼‚"""
        if coordination_score < 0.3:
            return 'significant'
        elif coordination_score < 0.5:
            return 'moderate'
        else:
            return 'minimal'
```

---

## 3. æŠ€æœ¯æ¶æ„è®¾è®¡

### 3.1 æ•°æ®é‡‡é›†å±‚

#### 3.1.1 æ•°æ®æºæ”¯æŒ

| å¹³å° | æ•°æ®æ ¼å¼ | é‡‡é›†æ–¹æ³• |
|-----|---------|---------|
| **å¾®ä¿¡** | SQLiteæ•°æ®åº“ã€CSVå¯¼å‡º | PyWxDumpã€WeChatMsg |
| **QQ** | NTæ•°æ®åº“ã€æ¶ˆæ¯å¤‡ä»½ | ç¬¬ä¸‰æ–¹è§£æå·¥å…· |
| **é’‰é’‰** | APIæ¥å£ã€ä¼ä¸šæ•°æ®å¯¼å‡º | ä¼ä¸šAPI |
| **é£ä¹¦** | APIæ¥å£ã€æ•°æ®å¯¼å‡º | ä¼ä¸šAPI |
| **WhatsApp** | åŠ å¯†æ•°æ®åº“ã€æ–‡æœ¬å¯¼å‡º | ç¬¬ä¸‰æ–¹å·¥å…· |
| **Telegram** | JSONå¯¼å‡º | TDLib |
| **Slack** | APIæ•°æ®å¯¼å‡º | Slack API |
| **Discord** | APIæ•°æ®å¯¼å‡º | Discord API |

#### 3.1.2 æ•°æ®æ¨¡å‹è®¾è®¡

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional, Dict
from enum import Enum

class MessageType(Enum):
    TEXT = "text"
    IMAGE = "image"
    VOICE = "voice"
    VIDEO = "video"
    FILE = "file"
    EMOJI = "emoji"
    SYSTEM = "system"

@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯æ•°æ®æ¨¡å‹"""
    message_id: str
    timestamp: datetime
    sender_id: str
    sender_name: str
    receiver_id: str
    receiver_name: str
    content: str
    message_type: MessageType
    channel: str  # å¾®ä¿¡ã€QQã€é’‰é’‰ç­‰
    
    # æ‰©å±•å­—æ®µ
    reply_to: Optional[str] = None  # å›å¤çš„æ¶ˆæ¯ID
    is_forwarded: bool = False
    has_mention: bool = False
    
    # åˆ†æç»“æœå­—æ®µ
    sentiment_score: Optional[float] = None
    emotion_type: Optional[str] = None
    keywords: List[str] = field(default_factory=list)
    topic_category: Optional[str] = None
    word_count: int = 0
    char_count: int = 0
    
    # å…ƒæ•°æ®
    device_type: Optional[str] = None
    location: Optional[Dict] = None

@dataclass 
class ChatSession:
    """èŠå¤©ä¼šè¯æ•°æ®æ¨¡å‹"""
    session_id: str
    participants: List[str]
    channel: str
    start_time: datetime
    end_time: Optional[datetime]
    message_count: int
    messages: List[ChatMessage]
    
    # ä¼šè¯åˆ†æç»“æœ
    average_response_time: float = 0.0
    dominant_topics: List[str] = field(default_factory=list)
    sentiment_trend: str = "neutral"
    activity_pattern: Dict = field(default_factory=dict)
```

### 3.2 æ•°æ®å¤„ç†å±‚

#### 3.2.1 æ•°æ®é¢„å¤„ç†ç®¡é“

```python
class DataPreprocessor:
    def __init__(self):
        self.stopwords = self._load_stopwords()
        self.emotion_dict = self._load_emotion_dict()
    
    def preprocess(self, raw_data: List[Dict]) -> List[ChatMessage]:
        """å®Œæ•´é¢„å¤„ç†æµç¨‹"""
        # 1. æ•°æ®æ¸…æ´—
        cleaned_data = self._clean_data(raw_data)
        
        # 2. æ ¼å¼æ ‡å‡†åŒ–
        standardized = self._standardize_format(cleaned_data)
        
        # 3. ç‰¹å¾æå–
        feature_extracted = self._extract_features(standardized)
        
        # 4. æ•°æ®éªŒè¯
        validated = self._validate_data(feature_extracted)
        
        return validated
    
    def _clean_data(self, data: List[Dict]) -> List[Dict]:
        """æ•°æ®æ¸…æ´—"""
        cleaned = []
        for item in data:
            # ç§»é™¤ç©ºæ¶ˆæ¯
            if not item.get('content', '').strip():
                continue
            
            # ç§»é™¤ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
            if item.get('message_type') == 'system':
                continue
            
            # ç§»é™¤é‡å¤æ¶ˆæ¯
            if self._is_duplicate(item, cleaned):
                continue
            
            # æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼
            item['timestamp'] = self._parse_timestamp(item.get('timestamp'))
            cleaned.append(item)
        
        return cleaned
    
    def _standardize_format(self, data: List[Dict]) -> List[Dict]:
        """æ ¼å¼æ ‡å‡†åŒ–"""
        standardized = []
        for item in data:
            standardized.append({
                'message_id': str(item.get('message_id', '')),
                'timestamp': item['timestamp'],
                'sender_id': str(item.get('sender_id', '')),
                'sender_name': str(item.get('sender_name', '')),
                'receiver_id': str(item.get('receiver_id', '')),
                'receiver_name': str(item.get('receiver_name', '')),
                'content': self._clean_text(item.get('content', '')),
                'message_type': MessageType(item.get('message_type', 'text')),
                'channel': str(item.get('channel', 'unknown')),
                'reply_to': item.get('reply_to'),
                'is_forwarded': item.get('is_forwarded', False),
                'has_mention': item.get('has_mention', False)
            })
        return standardized
    
    def _clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        # ç§»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹
        text = re.sub(r'[^\w\sï¼Œã€‚ï¼ï¼Ÿï¼šï¼›""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹\-\.\,]', '', text)
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _extract_features(self, data: List[Dict]) -> List[Dict]:
        """ç‰¹å¾æå–"""
        for item in data:
            content = item['content']
            
            # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
            item['word_count'] = len(jieba.lcut(content))
            item['char_count'] = len(content)
            
            # è¡¨æƒ…æ£€æµ‹
            item['has_emoji'] = bool(re.search(r'[ğŸ˜€-ğŸ™]', content))
            
            # æ ‡ç‚¹ç»Ÿè®¡
            item['exclamation_count'] = content.count('ï¼') + content.count('!')
            item['question_count'] = content.count('ï¼Ÿ') + content.count('?')
            item['period_count'] = content.count('ã€‚')
            
            # æ•æ„Ÿä¿¡æ¯æ£€æµ‹
            item['has_phone'] = bool(re.search(r'1[3-9]\d{9}', content))
            item['has_email'] = bool(re.search(r'\w+@\w+\.\w+', content))
        
        return data
    
    def _validate_data(self, data: List[Dict]) -> List[ChatMessage]:
        """æ•°æ®éªŒè¯"""
        validated = []
        for item in data:
            try:
                # éªŒè¯å¿…å¡«å­—æ®µ
                if not item['message_id'] or not item['timestamp']:
                    continue
                
                message = ChatMessage(**item)
                validated.append(message)
            except Exception as e:
                logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
                continue
        
        return validated
```

### 3.3 åˆ†æå¼•æ“å±‚

#### 3.3.1 åˆ†ææµæ°´çº¿æ¶æ„

```
åˆ†æå¼•æ“æ¶æ„ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åˆ†æè°ƒåº¦å™¨ (Orchestrator)                  â”‚
â”‚  - ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†                                               â”‚
â”‚  - å¹¶è¡Œæ‰§è¡Œæ§åˆ¶                                               â”‚
â”‚  - ç»“æœèšåˆ                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ—¶é—´åˆ†æå¼•æ“   â”‚ â”‚   å†…å®¹åˆ†æå¼•æ“   â”‚ â”‚   æƒ…æ„Ÿåˆ†æå¼•æ“   â”‚
â”‚ - æ´»è·ƒåº¦åˆ†æ    â”‚ â”‚ - ä¸»é¢˜æå–      â”‚ â”‚ - ææ€§åˆ†æ      â”‚
â”‚ - å“åº”é€Ÿåº¦      â”‚ â”‚ - å…³é”®è¯æå–    â”‚ â”‚ - æƒ…ç»ªåˆ†ç±»      â”‚
â”‚ - è¡Œä¸ºæ¨¡å¼      â”‚ â”‚ - è¯­ä¹‰ç›¸ä¼¼åº¦    â”‚ â”‚ - å¼ºåº¦è®¡ç®—      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ´å¯Ÿç”Ÿæˆå™¨ (Insight Generator)              â”‚
â”‚  - å¤šç»´äº¤å‰åˆ†æ                                               â”‚
â”‚  - å¼‚å¸¸æ£€æµ‹                                                   â”‚
â”‚  - è¶‹åŠ¿é¢„æµ‹                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.3.2 å¹¶è¡Œåˆ†ææ‰§è¡Œå™¨

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any

class ParallelAnalyzer:
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def analyze_parallel(self, messages: List[ChatMessage]) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æä»»åŠ¡"""
        loop = asyncio.get_event_loop()
        
        # å®šä¹‰åˆ†æä»»åŠ¡
        tasks = [
            loop.run_in_executor(self.executor, self._analyze_time, messages),
            loop.run_in_executor(self.executor, self._analyze_content, messages),
            loop.run_in_executor(self.executor, self._analyze_sentiment, messages),
            loop.run_in_executor(self.executor, self._analyze_social, messages),
            loop.run_in_executor(self.executor, self._analyze_personality, messages)
        ]
        
        # å¹¶è¡Œæ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return {
            'time_analysis': results[0] if not isinstance(results[0], Exception) else None,
            'content_analysis': results[1] if not isinstance(results[1], Exception) else None,
            'sentiment_analysis': results[2] if not isinstance(results[2], Exception) else None,
            'social_analysis': results[3] if not isinstance(results[3], Exception) else None,
            'personality_analysis': results[4] if not isinstance(results[4], Exception) else None
        }
    
    def _analyze_time(self, messages: List[ChatMessage]) -> Dict:
        """æ—¶é—´åˆ†æ"""
        # å®ç°æ—¶é—´åˆ†æé€»è¾‘
        pass
    
    def _analyze_content(self, messages: List[ChatMessage]) -> Dict:
        """å†…å®¹åˆ†æ"""
        # å®ç°å†…å®¹åˆ†æé€»è¾‘
        pass
    
    def _analyze_sentiment(self, messages: List[ChatMessage]) -> Dict:
        """æƒ…æ„Ÿåˆ†æ"""
        # å®ç°æƒ…æ„Ÿåˆ†æé€»è¾‘
        pass
    
    def _analyze_social(self, messages: List[ChatMessage]) -> Dict:
        """ç¤¾äº¤åˆ†æ"""
        # å®ç°ç¤¾äº¤åˆ†æé€»è¾‘
        pass
    
    def _analyze_personality(self, messages: List[ChatMessage]) -> Dict:
        """æ€§æ ¼åˆ†æ"""
        # å®ç°æ€§æ ¼åˆ†æé€»è¾‘
        pass
```

---

## 4. æ ¸å¿ƒç®—æ³•å®ç°

### 4.1 æ–‡æœ¬å¤„ç†æ¨¡å—

#### 4.1.1 ä¸­æ–‡åˆ†è¯ä¸è¯æ€§æ ‡æ³¨

```python
import jieba
import jieba.posseg as pseg
from typing import List, Tuple, Dict

class ChineseTextProcessor:
    def __init__(self):
        # åŠ è½½é¢†åŸŸè¯å…¸ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
        self.custom_dict_path = None
        if self.custom_dict_path:
            jieba.load_userdict(self.custom_dict_path)
        
        # åœç”¨è¯åˆ—è¡¨
        self.stopwords = self._load_stopwords()
    
    def segment(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        return list(jieba.cut(text))
    
    def segment_with_pos(self, text: str) -> List[Tuple[str, str]]:
        """åˆ†è¯+è¯æ€§æ ‡æ³¨"""
        return list(pseg.cut(text))
    
    def extract_nouns(self, text: str) -> List[str]:
        """æå–åè¯"""
        words = pseg.cut(text)
        return [word for word, flag in words if flag.startswith('n')]
    
    def extract_verbs(self, text: str) -> List[str]:
        """æå–åŠ¨è¯"""
        words = pseg.cut(text)
        return [word for word, flag in words if flag.startswith('v')]
    
    def remove_stopwords(self, words: List[str]) -> List[str]:
        """å»é™¤åœç”¨è¯"""
        return [w for w in words if w not in self.stopwords and len(w) > 1]
    
    def _load_stopwords(self) -> set:
        """åŠ è½½åœç”¨è¯"""
        # å¸¸è§ä¸­æ–‡åœç”¨è¯
        stopwords = {
            'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'åœ¨', 'æœ‰',
            'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´',
            'è¦', 'å»', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£'
        }
        return stopwords
```

### 4.2 å…³é”®è¯æå–æ¨¡å—

```python
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from collections import Counter
import numpy as np

class AdvancedKeywordExtractor:
    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95
        )
        self.count_vectorizer = CountVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            min_df=2
        )
        self.processor = ChineseTextProcessor()
    
    def extract_keywords(self, documents: List[str], method: str = 'hybrid', top_k: int = 20) -> List[Dict]:
        """ç»¼åˆå…³é”®è¯æå–"""
        if method == 'tfidf':
            return self._extract_tfidf(documents, top_k)
        elif method == 'frequency':
            return self._extract_frequency(documents, top_k)
        elif method == 'hybrid':
            return self._extract_hybrid(documents, top_k)
        elif method == 'textrank':
            return self._extract_textrank(documents, top_k)
    
    def _extract_tfidf(self, documents: List[str], top_k: int) -> List[Dict]:
        """TF-IDFå…³é”®è¯æå–"""
        processed_docs = []
        for doc in documents:
            words = self.processor.segment(doc)
            processed_docs.append(' '.join(self.processor.remove_stopwords(words)))
        
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_docs)
        feature_names = self.tfidf_vectorizer.get_feature_names_out()
        
        # è®¡ç®—æ¯ä¸ªè¯çš„å¹³å‡TF-IDFåˆ†æ•°
        mean_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()
        
        keywords = []
        for idx in mean_scores.argsort()[-top_k:][::-1]:
            keywords.append({
                'word': feature_names[idx],
                'score': float(mean_scores[idx]),
                'method': 'tfidf'
            })
        
        return keywords
    
    def _extract_frequency(self, documents: List[str], top_k: int) -> List[Dict]:
        """è¯é¢‘å…³é”®è¯æå–"""
        word_counter = Counter()
        
        for doc in documents:
            words = self.processor.segment(doc)
            words = self.processor.remove_stopwords(words)
            word_counter.update(words)
        
        keywords = []
        for word, count in word_counter.most_common(top_k):
            keywords.append({
                'word': word,
                'score': count,
                'method': 'frequency'
            })
        
        return keywords
    
    def _extract_hybrid(self, documents: List[str], top_k: int) -> List[Dict]:
        """æ··åˆæ–¹æ³•æå–å…³é”®è¯"""
        tfidf_results = self._extract_tfidf(documents, top_k)
        freq_results = self._extract_frequency(documents, top_k)
        
        # å½’ä¸€åŒ–åˆ†æ•°
        max_tfidf = max([k['score'] for k in tfidf_results]) if tfidf_results else 1
        max_freq = max([k['score'] for k in freq_results]) if freq_results else 1
        
        for k in tfidf_results:
            k['normalized_score'] = k['score'] / max_tfidf
        for k in freq_results:
            k['normalized_score'] = k['score'] / max_freq
        
        # åˆå¹¶ç»“æœ
        combined = {}
        for result in tfidf_results:
            combined[result['word']] = {
                'word': result['word'],
                'tfidf_score': result['score'],
                'normalized_tfidf': result['normalized_score'],
                'freq_score': 0,
                'normalized_freq': 0
            }
        for result in freq_results:
            if result['word'] in combined:
                combined[result['word']]['freq_score'] = result['score']
                combined[result['word']]['normalized_freq'] = result['normalized_score']
            else:
                combined[result['word']] = {
                    'word': result['word'],
                    'tfidf_score': 0,
                    'normalized_tfidf': 0,
                    'freq_score': result['score'],
                    'normalized_freq': result['normalized_freq']
                }
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        for word, data in combined.items():
            data['combined_score'] = (
                data['normalized_tfidf'] * 0.6 + 
                data['normalized_freq'] * 0.4
            )
        
        # æ’åºå¹¶è¿”å›
        sorted_keywords = sorted(
            combined.values(), 
            key=lambda x: x['combined_score'], 
            reverse=True
        )[:top_k]
        
        return sorted_keywords
    
    def _extract_textrank(self, documents: List[str], top_k: int) -> List[Dict]:
        """TextRankå…³é”®è¯æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # æ„å»ºè¯å…±ç°å›¾
        word_graph = {}
        window_size = 5
        
        for doc in documents:
            words = self.processor.segment(doc)
            words = self.processor.remove_stopwords(words)
            
            for i, word in enumerate(words):
                if word not in word_graph:
                    word_graph[word] = set()
                
                # çª—å£å†…çš„è¯å»ºç«‹è¾¹
                for j in range(max(0, i - window_size), min(len(words), i + window_size)):
                    if i != j:
                        word_graph[word].add(words[j])
        
        # è¿­ä»£è®¡ç®—PageRank
        damping = 0.85
        max_iter = 100
        tolerance = 0.0001
        
        scores = {word: 1.0 for word in word_graph}
        
        for _ in range(max_iter):
            new_scores = {}
            for word, neighbors in word_graph.items():
                score = (1 - damping) + damping * sum(
                    scores.get(neighbor, 0) / len(word_graph.get(neighbor, {word}))
                    for neighbor in neighbors
                )
                new_scores[word] = score
            
            # æ£€æŸ¥æ”¶æ•›
            diff = sum(abs(new_scores.get(word, 0) - scores.get(word, 0)) for word in scores)
            scores = new_scores
            
            if diff < tolerance:
                break
        
        # æ’åºè¿”å›
        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        return [
            {'word': word, 'score': score, 'method': 'textrank'}
            for word, score in sorted_words
        ]
```

### 4.3 æƒ…æ„Ÿåˆ†ææ¨¡å—

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np

class SentimentAnalyzer:
    """åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self, model_name='uer/roberta-base-finetuned-chinanese-english'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()
        
        self.emotion_lexicon = self._load_emotion_lexicon()
    
    def _load_emotion_lexicon(self) -> Dict[str, float]:
        """åŠ è½½æƒ…æ„Ÿè¯å…¸"""
        return {
            # ç§¯ææƒ…æ„Ÿè¯
            'å¼€å¿ƒ': 0.8, 'é«˜å…´': 0.8, 'å¿«ä¹': 0.9, 'å¹¸ç¦': 0.9, 'æ»¡æ„': 0.7,
            'å–œæ¬¢': 0.8, 'çˆ±': 0.9, 'æ£’': 0.9, 'ä¼˜ç§€': 0.8, 'å‰å®³': 0.7,
            # æ¶ˆææƒ…æ„Ÿè¯
            'éš¾è¿‡': -0.8, 'æ‚²ä¼¤': -0.8, 'ç”Ÿæ°”': -0.9, 'æ„¤æ€’': -0.9, 'å¤±æœ›': -0.7,
            'æ²®ä¸§': -0.7, 'ç„¦è™‘': -0.6, 'æ‹…å¿ƒ': -0.5, 'å®³æ€•': -0.6, 'è®¨åŒ': -0.8
        }
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """æƒ…æ„Ÿææ€§åˆ†æ"""
        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
        
        labels = ['negative', 'neutral', 'positive']
        pred_idx = probs.argmax().item()
        
        return {
            'sentiment': labels[pred_idx],
            'confidence': probs.max().item(),
            'scores': {
                'negative': probs[0][0].item(),
                'neutral': probs[0][1].item(),
                'positive': probs[0][2].item()
            }
        }
    
    def analyze_emotion(self, text: str) -> Dict[str, Any]:
        """ç»†ç²’åº¦æƒ…ç»ªåˆ†æ"""
        # åŸºäºè¯å…¸çš„æƒ…æ„Ÿè¯æå–
        words = jieba.lcut(text)
        emotion_scores = {}
        
        for word in words:
            if word in self.emotion_lexicon:
                emotion_scores[word] = self.emotion_lexicon[word]
        
        # è¡¨æƒ…ç¬¦å·åˆ†æ
        emoji_emotions = self._analyze_emojis(text)
        
        # æ ‡ç‚¹åˆ†æ
        punctuation_emotion = self._analyze_punctuation(text)
        
        # ç»¼åˆåˆ¤æ–­
        base_sentiment = self.analyze_sentiment(text)
        
        return {
            'base_sentiment': base_sentiment,
            'emotion_words': emotion_scores,
            'emoji_emotions': emoji_emotions,
            'punctuation_emotion': punctuation_emotion,
            'overall_emotion': self._calculate_overall_emotion(
                base_sentiment, emotion_scores, emoji_emotions, punctuation_emotion
            )
        }
    
    def _analyze_emojis(self, text: str) -> Dict[str, float]:
        """è¡¨æƒ…ç¬¦å·æƒ…æ„Ÿåˆ†æ"""
        emoji_sentiments = {
            'ğŸ˜€': 0.8, 'ğŸ˜„': 0.8, 'ğŸ˜Š': 0.7, 'ğŸ˜': 0.9, 'ğŸ¤”': 0.0,
            'ğŸ˜”': -0.3, 'ğŸ˜¢': -0.6, 'ğŸ˜­': -0.8, 'ğŸ˜ ': -0.8, 'ğŸ˜¡': -0.9,
            'ğŸ‘': 0.6, 'ğŸ‘': -0.5, 'â¤ï¸': 0.8, 'ğŸ’”': -0.8
        }
        
        found_emojis = {}
        for emoji, sentiment in emoji_sentiments.items():
            if emoji in text:
                found_emojis[emoji] = sentiment
        
        return found_emojis
    
    def _analyze_punctuation(self, text: str) -> Dict[str, Any]:
        """æ ‡ç‚¹ç¬¦å·æƒ…æ„Ÿåˆ†æ"""
        result = {
            'exclamation_count': text.count('ï¼') + text.count('!'),
            'question_count': text.count('ï¼Ÿ') + text.count('?'),
            'ellipsis_count': text.count('â€¦â€¦') + text.count('...'),
            'sentiment_impact': 0.0
        }
        
        # è®¡ç®—æ ‡ç‚¹å¯¹æƒ…æ„Ÿçš„å¢å¼ºæ•ˆæœ
        result['sentiment_impact'] = min(1.0, (
            result['exclamation_count'] * 0.1 + 
            result['question_count'] * 0.05 +
            result['ellipsis_count'] * 0.08
        ))
        
        return result
    
    def _calculate_overall_emotion(
        self, 
        base_sentiment: Dict,
        emotion_words: Dict,
        emoji_emotions: Dict,
        punctuation_emotion: Dict
    ) -> Dict[str, Any]:
        """ç»¼åˆè®¡ç®—æ•´ä½“æƒ…ç»ª"""
        # åŸºç¡€æƒ…æ„Ÿåˆ†æ•° (-1åˆ°1)
        base_score = (base_sentiment['scores']['positive'] - 
                     base_sentiment['scores']['negative'])
        
        # æƒ…æ„Ÿè¯è´¡çŒ®
        if emotion_words:
            word_score = sum(emotion_words.values()) / len(emotion_words)
        else:
            word_score = 0
        
        # è¡¨æƒ…è´¡çŒ®
        if emoji_emotions:
            emoji_score = sum(emoji_emotions.values()) / len(emoji_emotions)
        else:
            emoji_score = 0
        
        # ç»¼åˆè®¡ç®—
        overall_score = (
            base_score * 0.5 + 
            word_score * 0.3 + 
            emoji_score * 0.2
        )
        
        # æ ¹æ®æ ‡ç‚¹è°ƒæ•´
        overall_score += punctuation_emotion['sentiment_impact'] * 0.05 * (1 if overall_score > 0 else -1)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        overall_score = max(-1.0, min(1.0, overall_score))
        
        # ç¡®å®šæƒ…ç»ªç±»å‹
        if overall_score > 0.3:
            emotion_type = 'ç§¯æ'
        elif overall_score < -0.3:
            emotion_type = 'æ¶ˆæ'
        elif abs(overall_score) <= 0.3:
            emotion_type = 'ä¸­æ€§'
        
        return {
            'score': overall_score,
            'type': emotion_type,
            'intensity': abs(overall_score),
            'confidence': base_sentiment['confidence']
        }
```

### 4.4 ä¸»é¢˜å»ºæ¨¡æ¨¡å—

```python
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

class TopicModeler:
    """ä¸»é¢˜å»ºæ¨¡åˆ†æå™¨"""
    
    def __init__(self, n_topics=10):
        self.n_topics = n_topics
        self.tfidf_vectorizer = TfidfVectorizer(
            max_df=0.95,
            min_df=2,
            max_features=2000,
            ngram_range=(1, 2)
        )
        self.lda_model = LatentDirichletAllocation(
            n_components=n_topics,
            max_iter=20,
            learning_method='online',
            random_state=42
        )
        self.nmf_model = NMF(
            n_components=n_topics,
            random_state=42,
            max_iter=200
        )
        self.processor = ChineseTextProcessor()
    
    def extract_topics_lda(self, documents: List[str], top_n=10) -> List[Dict]:
        """LDAä¸»é¢˜æå–"""
        # é¢„å¤„ç†
        processed_docs = []
        for doc in documents:
            words = self.processor.segment(doc)
            words = self.processor.remove_stopwords(words)
            processed_docs.append(' '.join(words))
        
        # TF-IDFè½¬æ¢
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_docs)
        
        # LDAå»ºæ¨¡
        doc_topic_dist = self.lda_model.fit_transform(tfidf_matrix)
        
        # æå–ä¸»é¢˜è¯
        feature_names = self.tfidf_vectorizer.get_feature_names_out()
        topics = []
        
        for topic_idx, topic in enumerate(self.lda_model.components_):
            topic_words = [feature_names[i] for i in topic.argsort()[:-top_n-1:-1]]
            topic_weight = doc_topic_dist[:, topic_idx].mean()
            
            topics.append({
                'topic_id': topic_idx,
                'words': topic_words,
                'weight': float(topic_weight),
                'distribution': doc_topic_dist[:, topic_idx].tolist()
            })
        
        return topics
    
    def track_topic_evolution(self, documents: List[str], timestamps: List[str], window_size: int = 100) -> List[Dict]:
        """è¿½è¸ªä¸»é¢˜éšæ—¶é—´çš„æ¼”å˜"""
        # æŒ‰æ—¶é—´æ’åºæ–‡æ¡£
        sorted_indices = np.argsort(timestamps)
        sorted_docs = [documents[i] for i in sorted_indices]
        sorted_times = [timestamps[i] for i in sorted_indices]
        
        # æ»‘åŠ¨çª—å£
        evolution_data = []
        for i in range(0, len(sorted_docs), window_size):
            window_docs = sorted_docs[i:i + window_size]
            window_time = sorted_times[i] if sorted_times else None
            
            if len(window_docs) < 10:  # è·³è¿‡å¤ªå°çš„çª—å£
                continue
            
            topics = self.extract_topics_lda(window_docs)
            
            evolution_data.append({
                'timestamp': window_time,
                'topics': topics,
                'message_count': len(window_docs)
            })
        
        return evolution_data
    
    def get_document_topics(self, document: str) -> List[Dict]:
        """è·å–å•æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ"""
        words = self.processor.segment(document)
        words = self.processor.remove_stopwords(words)
        processed = ' '.join(words)
        
        tfidf = self.tfidf_vectorizer.transform([processed])
        topic_dist = self.lda_model.transform(tfidf)[0]
        
        return [
            {'topic_id': i, 'weight': float(weight)}
            for i, weight in enumerate(topic_dist)
            if weight > 0.01
        ]
```

---

## 5. æ€§æ ¼åˆ†ææ–¹æ³•è®º

### 5.1 å¤§äº”äººæ ¼è¯­è¨€å­¦æ˜ å°„

åŸºäºLIWCï¼ˆLinguistic Inquiry and Word Countï¼‰ç­‰å·¥å…·çš„ç ”ç©¶ï¼Œå‘ç°ç‰¹å®šäººæ ¼ç‰¹è´¨ä¸è¯æ±‡ç±»åˆ«ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚

#### 5.1.1 å¤§äº”äººæ ¼ç‰¹å¾è¯†åˆ«

| äººæ ¼ç‰¹è´¨ | è¯­è¨€ç‰¹å¾ | è¡Œä¸ºè¡¨ç° | ä¸­æ–‡è¯­å¢ƒè¡¨ç° |
|---------|---------|---------|-------------|
| **å¤–å‘æ€§** | ç¤¾äº¤è¯æ±‡é«˜é¢‘ã€ç§¯ææƒ…ç»ªè¯ã€è¡¨æƒ…ç¬¦å·ä¸°å¯Œ | é«˜ç¤¾äº¤æ´»è·ƒåº¦ã€è¯é¢˜å‘èµ·è€… | ä½¿ç”¨"æœ‹å‹"ã€"è§é¢"ç­‰è¯ï¼Œè™šè¯ä½¿ç”¨è¾ƒå°‘ |
| **å®œäººæ€§** | åŒ…å«æ€§è¯æ±‡ï¼ˆæˆ‘ä»¬ã€ä¸€èµ·ï¼‰ã€è‚¯å®šæ€§è¡¨è¾¾ã€èµç¾è¯ | åˆä½œå¯¼å‘ã€å…±æƒ…èƒ½åŠ›å¼º | é«˜å®œäººè€…ä½¿ç”¨è¾ƒå¤šäººä½“è¯å’Œç§¯ææƒ…ç»ªè¯ |
| **å°½è´£æ€§** | æˆå°±/ç§©åº/å·¥ä½œè¯æ±‡ã€æ•°å­—é‡åŒ–è¯æ±‡ | è®¡åˆ’æ€§å¼ºã€æ³¨é‡ç»†èŠ‚ | æ–‡æœ¬é€»è¾‘æ¸…æ™°ï¼Œä¸è®¤çŸ¥è¿‡ç¨‹è¯æ­£ç›¸å…³ |
| **ç¥ç»è´¨** | ç¬¬ä¸€äººç§°"æˆ‘"ã€æ¶ˆææƒ…ç»ªè¯ã€ä¸ç¡®å®šæ€§è¡¨è¾¾ | æƒ…ç»ªæ³¢åŠ¨å¤§ã€è‡ªæˆ‘å…³æ³¨ | é¢‘ç¹ä½¿ç”¨"ç„¦è™‘"ã€"æ‹…å¿ƒ"ç­‰è¯ |
| **å¼€æ”¾æ€§** | è¯æ±‡ä¸°å¯Œã€å¥å¼å¤æ‚ã€æŠ½è±¡æ¦‚å¿µè®¨è®º | æ±‚çŸ¥æ¬²å¼ºã€æ¥å—æ–°äº‹ç‰© | å¸¸è®¨è®ºæ–‡åŒ–è¯é¢˜ï¼Œè¯­è¨€å¤æ‚åº¦é«˜ |

#### 5.1.2 æ€§æ ¼åˆ†æå®ç°

```python
from collections import Counter
import re

class PersonalityAnalyzer:
    """åŸºäºè¯­è¨€æ¨¡å¼çš„æ€§æ ¼åˆ†æå™¨"""
    
    def __init__(self):
        # å¤§äº”äººæ ¼è¯å…¸
        self.big5_lexicons = {
            'extraversion': {
                'social_words': ['æœ‹å‹', 'èŠå¤©', 'è§é¢', 'èšä¼š', 'ä¸€èµ·', 'å¤§å®¶'],
                'positive_emotion': ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å…´å¥‹', 'æ¿€åŠ¨'],
                'emoji_pattern': r'[ğŸ˜€-ğŸ™]',
                'exclamation_pattern': r'[ï¼!]'
            },
            'agreeableness': {
                'inclusive_words': ['æˆ‘ä»¬', 'ä¸€èµ·', 'å’±ä»¬', 'å¤§å®¶'],
                'affirmative': ['æ˜¯çš„', 'å¯¹çš„', 'å¥½', 'åŒæ„', 'èµæˆ'],
                'praise': ['å‰å®³', 'ä¼˜ç§€', 'æ£’', 'èµ', 'ç‰›']
            },
            'conscientiousness': {
                'achievement': ['å·¥ä½œ', 'ä»»åŠ¡', 'å®Œæˆ', 'è®¡åˆ’', 'ç›®æ ‡', 'æˆªæ­¢'],
                'order': ['é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æœ€å', 'æ•´ç†'],
                'quantitative': ['ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'æ•°å­—', 'å…·ä½“']
            },
            'neuroticism': {
                'self_reference': ['æˆ‘', 'æˆ‘çš„', 'è‡ªå·±'],
                'negative_emotion': ['ç„¦è™‘', 'æ‹…å¿ƒ', 'å®³æ€•', 'ç”Ÿæ°”', 'éš¾è¿‡', 'å‹åŠ›'],
                'uncertainty': ['å¯èƒ½', 'å¤§æ¦‚', 'ä¹Ÿè®¸', 'ä¸çŸ¥é“', 'å¥½åƒ']
            },
            'openness': {
                'abstract_concepts': ['æ„ä¹‰', 'æƒ³æ³•', 'æ€è€ƒ', 'å“²å­¦', 'æ–‡åŒ–'],
                'variety': ['ä¸åŒ', 'å„ç§', 'å¤šæ ·', 'æ–°é²œ', 'æ–°'],
                'intellectual': ['ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'ç†è§£', 'åˆ†æ', 'ç ”ç©¶']
            }
        }
        
        self.processor = ChineseTextProcessor()
    
    def analyze_big5(self, messages: List[ChatMessage]) -> Dict[str, float]:
        """åˆ†æå¤§äº”äººæ ¼ç‰¹è´¨"""
        all_text = ' '.join([msg.content for msg in messages])
        words = self.processor.segment(all_text)
        word_count = len(words)
        
        if word_count == 0:
            return {trait: 0.0 for trait in self.big5_lexicons.keys()}
        
        scores = {}
        
        for trait, lexicon in self.big5_lexicons.items():
            score = 0.0
            
            if 'social_words' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['social_words']) / word_count
            
            if 'positive_emotion' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['positive_emotion']) / word_count
            
            if 'emoji_pattern' in lexicon:
                emoji_count = len(re.findall(lexicon['emoji_pattern'], all_text))
                score += emoji_count / (word_count / 10)  # å½’ä¸€åŒ–
            
            if 'exclamation_pattern' in lexicon:
                exc_count = len(re.findall(lexicon['exclamation_pattern'], all_text))
                score += exc_count / (word_count / 50)
            
            if 'inclusive_words' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['inclusive_words']) / word_count
            
            if 'affirmative' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['affirmative']) / word_count
            
            if 'praise' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['praise']) / word_count
            
            if 'achievement' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['achievement']) / word_count
            
            if 'order' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['order']) / word_count
            
            if 'quantitative' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['quantitative']) / word_count
            
            if 'self_reference' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['self_reference']) / word_count
            
            if 'negative_emotion' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['negative_emotion']) / word_count
            
            if 'uncertainty' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['uncertainty']) / word_count
            
            if 'abstract_concepts' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['abstract_concepts']) / word_count
            
            if 'variety' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['variety']) / word_count
            
            if 'intellectual' in lexicon:
                score += sum(all_text.count(w) for w in lexicon['intellectual']) / word_count
            
            # å½’ä¸€åŒ–åˆ†æ•°åˆ°0-1
            scores[trait] = min(1.0, score * 2)
        
        return scores
    
    def get_personality_report(self, messages: List[ChatMessage]) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„æ€§æ ¼åˆ†ææŠ¥å‘Š"""
        big5_scores = self.analyze_big5(messages)
        
        # è¯­è¨€é£æ ¼åˆ†æ
        language_style = self._analyze_language_style(messages)
        
        # ç¤¾äº¤è¡Œä¸ºåˆ†æ
        social_behavior = self._analyze_social_behavior(messages)
        
        # æ‰“å­—è¡Œä¸ºåˆ†æ
        typing_behavior = self._analyze_typing_behavior(messages)
        
        return {
            'big5_personality': big5_scores,
            'language_style': language_style,
            'social_behavior': social_behavior,
            'typing_behavior': typing_behavior,
            'interpretation': self._generate_interpretation(big5_scores)
        }
    
    def _analyze_language_style(self, messages: List[ChatMessage]) -> Dict:
        """åˆ†æè¯­è¨€é£æ ¼"""
        all_text = ' '.join([msg.content for msg in messages])
        total_messages = len(messages)
        
        if total_messages == 0:
            return {}
        
        # æ„Ÿå¹è¯åˆ†æ
        exclamations = ['å‘€', 'å‘µ', 'å“ˆå“ˆ', 'å—¯', 'å“¦', 'å•Š']
        exclamation_usage = {e: all_text.count(e) / total_messages for e in exclamations}
        
        # æ ‡ç‚¹ä½¿ç”¨
        punctuation_stats = {
            'period_ratio': all_text.count('ã€‚') / len(all_text),
            'exclamation_ratio': (all_text.count('ï¼') + all_text.count('!')) / len(all_text),
            'question_ratio': (all_text.count('ï¼Ÿ') + all_text.count('?')) / len(all_text)
        }
        
        # è¡¨æƒ…ä½¿ç”¨
        emoji_count = len(re.findall(r'[ğŸ˜€-ğŸ™]', all_text))
        emoji_ratio = emoji_count / total_messages
        
        return {
            'exclamation_usage': exclamation_usage,
            'punctuation_stats': punctuation_stats,
            'emoji_ratio': emoji_ratio,
            'avg_message_length': np.mean([len(msg.content) for msg in messages])
        }
    
    def _analyze_social_behavior(self, messages: List[ChatMessage]) -> Dict:
        """åˆ†æç¤¾äº¤è¡Œä¸º"""
        senders = Counter([msg.sender_id for msg in messages])
        total_messages = len(messages)
        
        # ä¸»åŠ¨å‘èµ·æ¯”ä¾‹
        initiation_rate = len(messages) / total_messages if total_messages > 0 else 0
        
        # äº’åŠ¨å¹¿åº¦
        unique_contacts = len(senders)
        
        # äº’åŠ¨æ·±åº¦ï¼ˆå¹³å‡æ¶ˆæ¯é•¿åº¦ï¼‰
        avg_depth = np.mean([len(msg.content) for msg in messages])
        
        return {
            'initiation_rate': initiation_rate,
            'unique_contacts': unique_contacts,
            'interaction_depth': avg_depth
        }
    
    def _analyze_typing_behavior(self, messages: List[ChatMessage]) -> Dict:
        """åˆ†ææ‰“å­—è¡Œä¸º"""
        if not messages:
            return {}
        
        # é”™å­—/ä¿®æ­£æ ‡å¿—ï¼ˆå¦‚"xx"è¡¨ç¤ºä¿®æ­£ï¼‰
        correction_pattern = r'[(ï¼ˆ][^ï¼‰)]*[)ï¼‰]'  # æ‹¬å·å†…çš„å†…å®¹é€šå¸¸æ˜¯ä¿®æ­£
        
        corrections = []
        for msg in messages:
            found = re.findall(correction_pattern, msg.content)
            corrections.extend(found)
        
        return {
            'correction_count': len(corrections),
            'correction_rate': len(corrections) / len(messages) if messages else 0
        }
    
    def _generate_interpretation(self, scores: Dict[str, float]) -> Dict[str, str]:
        """ç”Ÿæˆæ€§æ ¼è§£è¯»"""
        interpretations = {}
        
        if scores['extraversion'] > 0.6:
            interpretations['extraversion'] = "å¤–å‘å¼€æœ—ï¼Œç¤¾äº¤æ´»è·ƒåº¦é«˜ï¼Œå–„äºå‘èµ·å’Œä¸»å¯¼å¯¹è¯"
        elif scores['extraversion'] < 0.4:
            interpretations['extraversion'] = "ç›¸å¯¹å†…å‘ï¼Œå€¾å‘äºæ·±åº¦äº¤æµè€Œéå¹¿æ³›ç¤¾äº¤"
        else:
            interpretations['extraversion'] = "ä¸­ç­‰å¤–å‘ï¼Œç¤¾äº¤è¡Œä¸ºå¹³è¡¡"
        
        if scores['agreeableness'] > 0.6:
            interpretations['agreeableness'] = "å®œäººæ€§é«˜ï¼Œæ³¨é‡äººé™…å…³ç³»å’Œè°ï¼Œå…·æœ‰è¾ƒå¼ºçš„å…±æƒ…èƒ½åŠ›"
        elif scores['agreeableness'] < 0.4:
            interpretations['agreeableness'] = "ç›¸å¯¹åšæŒå·±è§ï¼Œå¯èƒ½åœ¨å†³ç­–ä¸­è¾ƒä¸ºæœæ–­"
        else:
            interpretations['agreeableness'] = "å®œäººæ€§ä¸­ç­‰ï¼Œèƒ½å¤Ÿå¹³è¡¡è‡ªèº«éœ€æ±‚ä¸ä»–äººæœŸæœ›"
        
        if scores['conscientiousness'] > 0.6:
            interpretations['conscientiousness'] = "å°½è´£æ€§é«˜ï¼Œåšäº‹æœ‰æ¡ç†ï¼Œè®¡åˆ’æ€§å¼ºï¼Œæ³¨é‡ç»†èŠ‚"
        elif scores['conscientiousness'] < 0.4:
            interpretations['conscientiousness'] = "ç›¸å¯¹éšæ„ï¼Œçµæ´»åº”å˜ï¼Œä¸å¤ªæ‹˜æ³¥äºè§„åˆ™"
        else:
            interpretations['conscientiousness'] = "å°½è´£æ€§ä¸­ç­‰ï¼Œèƒ½å¤Ÿåœ¨è®¡åˆ’ä¸çµæ´»ä¹‹é—´æ‰¾åˆ°å¹³è¡¡"
        
        if scores['neuroticism'] > 0.6:
            interpretations['neuroticism'] = "ç¥ç»è´¨è¾ƒé«˜ï¼Œæƒ…ç»ªæ³¢åŠ¨å¯èƒ½è¾ƒå¤§ï¼Œå¯¹å‹åŠ›è¾ƒæ•æ„Ÿ"
        elif scores['neuroticism'] < 0.4:
            interpretations['neuroticism'] = "æƒ…ç»ªç¨³å®šæ€§å¥½ï¼Œèƒ½å¤Ÿè¾ƒå¥½åœ°åº”å¯¹å‹åŠ›"
        else:
            interpretations['neuroticism'] = "æƒ…ç»ªç¨³å®šæ€§ä¸­ç­‰ï¼Œèƒ½å¤Ÿä¿æŒç›¸å¯¹å¹³è¡¡çš„å¿ƒæ€"
        
        if scores['openness'] > 0.6:
            interpretations['openness'] = "å¼€æ”¾æ€§é«˜ï¼Œæ€ç»´æ´»è·ƒï¼Œå¯¹æ–°äº‹ç‰©æ¥å—åº¦é«˜"
        elif scores['openness'] < 0.4:
            interpretations['openness'] = "ç›¸å¯¹åŠ¡å®ï¼Œæ›´å€¾å‘äºç†Ÿæ‚‰çš„é¢†åŸŸå’Œæ–¹å¼"
        else:
            interpretations['openness'] = "å¼€æ”¾æ€§ä¸­ç­‰ï¼Œå¯¹æ–°äº‹ç‰©æŒå¼€æ”¾ä½†å®¡æ…çš„æ€åº¦"
        
        return interpretations
```

### 5.2 MBTIç»´åº¦åˆ†æ

```python
class MBTIAnalyzer:
    """MBTIäººæ ¼ç»´åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.t_f_patterns = {
            'thinking': ['å› ä¸º', 'æ‰€ä»¥', 'å› æ­¤', 'é€»è¾‘', 'é“ç†', 'åˆ†æ', 'ç†æ€§', 'åº”è¯¥', 'åˆç†'],
            'feeling': ['æ„Ÿè§‰', 'æƒ…æ„Ÿ', 'å–œæ¬¢', 'åœ¨ä¹', 'æ‹…å¿ƒ', 'å®³æ€•', 'æ¸©æŸ”', 'ç†è§£']
        }
        
        self.j_p_patterns = {
            'judging': ['è®¡åˆ’', 'å®‰æ’', 'å†³å®š', 'ç¡®å®š', 'å®Œæˆ', 'æˆªæ­¢', 'åº”è¯¥', 'å¿…é¡»', 'æŒ‰æ—¶'],
            'perceiving': ['å¯èƒ½', 'éšä¾¿', 'åˆ°æ—¶å€™', 'å†è¯´', 'çœ‹æƒ…å†µ', 'çµæ´»', 'éšæ—¶']
        }
    
    def analyze_dimension(self, messages: List[ChatMessage]) -> Dict[str, Dict]:
        """åˆ†æMBTIå„ç»´åº¦"""
        all_text = ' '.join([msg.content for msg in messages])
        total_words = len(all_text)
        
        results = {}
        
        # E vs I
        results['E_I'] = self._analyze_ei(messages, all_text, total_words)
        
        # S vs N  
        results['S_N'] = self._analyze_sn(messages, all_text, total_words)
        
        # T vs F
        results['T_F'] = self._analyze_tf(messages, all_text, total_words)
        
        # J vs P
        results['J_P'] = self._analyze_jp(messages, all_text, total_words)
        
        return results
    
    def _analyze_ei(self, messages: List[ChatMessage], all_text: str, total_words: int) -> Dict:
        """åˆ†æå¤–å‘-å†…å‘ç»´åº¦"""
        social_score = sum(all_text.count(w) for w in ['æˆ‘ä»¬', 'å¤§å®¶', 'ä¸€èµ·', 'æœ‹å‹']) / total_words * 100
        solitary_score = sum(all_text.count(w) for w in ['è‡ªå·±', 'å•ç‹¬', 'å®‰é™', 'ä¸€ä¸ªäºº']) / total_words * 100
        
        # æ¶ˆæ¯å‘èµ·é¢‘ç‡
        initiation_count = len(messages)
        avg_initiation = initiation_count / len(set(msg.sender_id for msg in messages))
        
        e_score = min(1.0, (social_score * 0.6 + avg_initiation * 0.4))
        
        return {
            'E_score': e_score,
            'I_score': 1 - e_score,
            'tendency': 'E' if e_score > 0.55 else ('I' if e_score < 0.45 else 'Ambivert'),
            'social_markers': social_score,
            'solitary_markers': solitary_score
        }
    
    def _analyze_sn(self, messages: List[ChatMessage], all_text: str, total_words: int) -> Dict:
        """åˆ†ææ„Ÿè§‰-ç›´è§‰ç»´åº¦"""
        sensory_score = sum(all_text.count(w) for w in ['çœ‹è§', 'å¬åˆ°', 'æ„Ÿè§‰', 'å…·ä½“', 'å®é™…']) / total_words * 100
        intuitive_score = sum(all_text.count(w) for w in ['å¯èƒ½', 'æœªæ¥', 'æƒ³è±¡', 'å¤§æ¦‚', 'æ€»è§‰å¾—']) / total_words * 100
        
        n_score = intuitive_score / (sensory_score + intuitive_score + 0.001)
        
        return {
            'S_score': 1 - n_score,
            'N_score': n_score,
            'tendency': 'N' if n_score > 0.55 else ('S' if n_score < 0.45 else 'Slight N'),
            'sensory_markers': sensory_score,
            'intuitive_markers': intuitive_score
        }
    
    def _analyze_tf(self, messages: List[ChatMessage], all_text: str, total_words: int) -> Dict:
        """åˆ†ææ€ç»´-æƒ…æ„Ÿç»´åº¦"""
        thinking_count = sum(all_text.count(w) for w in self.t_f_patterns['thinking'])
        feeling_count = sum(all_text.count(w) for w in self.t_f_patterns['feeling'])
        
        t_score = thinking_count / (thinking_count + feeling_count + 0.001)
        
        return {
            'T_score': t_score,
            'F_score': 1 - t_score,
            'tendency': 'T' if t_score > 0.55 else ('F' if t_score < 0.45 else 'Balanced'),
            'thinking_markers': thinking_count,
            'feeling_markers': feeling_count
        }
    
    def _analyze_jp(self, messages: List[ChatMessage], all_text: str, total_words: int) -> Dict:
        """åˆ†æåˆ¤æ–­-çŸ¥è§‰ç»´åº¦"""
        judging_count = sum(all_text.count(w) for w in self.j_p_patterns['judging'])
        perceiving_count = sum(all_text.count(w) for w in self.j_p_patterns['perceiving'])
        
        j_score = judging_count / (judging_count + perceiving_count + 0.001)
        
        return {
            'J_score': j_score,
            'P_score': 1 - j_score,
            'tendency': 'J' if j_score > 0.55 else ('P' if j_score < 0.45 else 'Flexible'),
            'judging_markers': judging_count,
            'perceiving_markers': perceiving_count
        }
    
    def get_mbti_type(self, messages: List[ChatMessage]) -> str:
        """æ¨æ–­MBTIç±»å‹"""
        dimensions = self.analyze_dimension(messages)
        
        e_i = 'E' if dimensions['E_I']['tendency'] == 'E' else 'I'
        s_n = 'S' if dimensions['S_N']['tendency'] == 'S' else 'N'
        t_f = 'T' if dimensions['T_F']['tendency'] == 'T' else 'F'
        j_p = 'J' if dimensions['J_P']['tendency'] == 'J' else 'P'
        
        return e_i + s_n + t_f + j_p
```

---

## 6. ç¤¾ä¼šå·¥ç¨‹å­¦ä¿¡æ¯æå–

### 6.1 ä¸ªäººä¿¡æ¯è¯†åˆ«ä¸æå–

```python
import re
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass
class PersonalInfo:
    """ä¸ªäººä¿¡æ¯æ•°æ®ç»“æ„"""
    phones: List[str] = field(default_factory=list)
    emails: List[str] = field(default_factory=list)
    names: List[str] = field(default_factory=list)
    ids: List[str] = field(default_factory=list)
    addresses: List[str] = field(default_factory=list)
    bank_accounts: List[str] = field(default_factory=list)

class PIIDetector:
    """ä¸ªäººèº«ä»½ä¿¡æ¯æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.patterns = {
            'phone': r'(1[3-9]\d{9})',
            'email': r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
            'id_card': r'([1-9]\d{5}(18|19|20)\d{2}(0[1-9]|1[0-2])(0[1-9]|[1-2]\d|3[0-1])\d{3}[\dXx])',
            'bank_card': r'([1-9]\d{15,18})',
            'passport': r'([EKG]1\d{7,9})',
            'hk_taiwan': r'([HM]\d{10})'
        }
        
        # æ•æ„Ÿå…³é”®è¯
        self.sensitive_keywords = {
            'location': ['ä½', 'å®¶', 'åœ°å€', 'ä½ç½®', 'å…¬å¸åœ°å€', 'å®¶åº­ä½å€'],
            'identity': ['èº«ä»½è¯', 'æŠ¤ç…§', 'é©¾ç…§', 'ç¤¾ä¿', 'åŒ»ä¿'],
            'financial': ['é“¶è¡Œå¡', 'ä¿¡ç”¨å¡', 'å¯†ç ', 'è´¦æˆ·', 'æ”¯ä»˜'],
            'health': ['ç—…', 'åŒ»é™¢', 'ä½“æ£€', 'å¥åº·'],
            'contact': ['æ‰‹æœº', 'ç”µè¯', 'é‚®ç®±', 'å¾®ä¿¡', 'QQ']
        }
    
    def detect_pii(self, text: str) -> PersonalInfo:
        """æ£€æµ‹æ–‡æœ¬ä¸­çš„PIIä¿¡æ¯"""
        info = PersonalInfo()
        
        for pii_type, pattern in self.patterns.items():
            matches = re.findall(pattern, text)
            if pii_type == 'phones':
                info.phones.extend(matches)
            elif pii_type == 'emails':
                info.emails.extend(matches)
            elif pii_type == 'id_card':
                info.ids.extend(matches)
            elif pii_type == 'bank_card':
                info.bank_accounts.extend(matches)
        
        # æ£€æµ‹æ•æ„Ÿå…³é”®è¯
        info.sensitive_keywords_found = self._detect_sensitive_keywords(text)
        
        return info
    
    def _detect_sensitive_keywords(self, text: str) -> Dict[str, List[str]]:
        """æ£€æµ‹æ•æ„Ÿå…³é”®è¯"""
        found = {}
        for category, keywords in self.sensitive_keywords.items():
            matched = []
            for keyword in keywords:
                if keyword in text:
                    matched.append(keyword)
            if matched:
                found[category] = matched
        return found
    
    def assess_privacy_risk(self, pii_info: PersonalInfo) -> Dict[str, Any]:
        """è¯„ä¼°éšç§æ³„éœ²é£é™©"""
        risk_score = 0
        risk_factors = []
        
        if pii_info.phones:
            risk_score += 30
            risk_factors.append('æ‰‹æœºå·æ³„éœ²')
        
        if pii_info.emails:
            risk_score += 20
            risk_factors.append('é‚®ç®±åœ°å€æ³„éœ²')
        
        if pii_info.ids:
            risk_score += 50
            risk_factors.append('èº«ä»½è¯ä¿¡æ¯æ³„éœ²')
        
        if pii_info.bank_accounts:
            risk_score += 60
            risk_factors.append('é“¶è¡Œå¡ä¿¡æ¯æ³„éœ²')
        
        risk_level = 'high' if risk_score > 50 else ('medium' if risk_score > 20 else 'low')
        
        return {
            'risk_score': min(100, risk_score),
            'risk_level': risk_level,
            'risk_factors': risk_factors
        }
```

### 6.2 å¨èƒæ£€æµ‹ä¸è¡Œä¸ºåˆ†æ

```python
class SocialEngineeringDetector:
    """ç¤¾ä¼šå·¥ç¨‹å­¦æ”»å‡»æ£€æµ‹å™¨"""
    
    def __init__(self):
        # æ”»å‡»æ¨¡å¼æ£€æµ‹
        self.attack_patterns = {
            'urgency': [r'ç«‹å³', r'é©¬ä¸Š', r'ç«‹åˆ»', r'é™æ—¶', r'24å°æ—¶', r'å°½å¿«'],
            'authority': [r'å®˜æ–¹', r'å®¢æœ', r'é¢†å¯¼', r'ç»ç†', r'ç³»ç»Ÿç®¡ç†å‘˜'],
            'curiosity': [r'æ‚¨æœ‰ä¸€æ¡', r'æ‚¨æœ‰', r'æ­å–œæ‚¨', r'ä¸­å¥–'],
            'fear': [r'è¿æ³•', r'å†»ç»“', r'å°å·', r'èµ·è¯‰', r'è¿½ç©¶'],
            'trust_exploitation': [r'ç†Ÿäºº', r'è€åŒå­¦', r'åŒäº‹', r'å¸®ä¸ªå¿™']
        }
        
        # å¯ç–‘è¯·æ±‚æ¨¡å¼
        self.suspicious_requests = [
            r'éªŒè¯ç ',
            r'å¯†ç ',
            r'è½¬è´¦',
            r'æ±‡æ¬¾',
            r'ç‚¹å‡»é“¾æ¥',
            r'ä¸‹è½½æ–‡ä»¶'
        ]
    
    def detect_threats(self, messages: List[ChatMessage]) -> List[Dict]:
        """æ£€æµ‹èŠå¤©ä¸­çš„å¨èƒä¿¡å·"""
        threats = []
        
        for msg in messages:
            content = msg.content
            
            # æ£€æµ‹æ”»å‡»æ¨¡å¼
            attack_indicators = self._detect_attack_patterns(content)
            
            # æ£€æµ‹å¯ç–‘è¯·æ±‚
            suspicious_requests = self._detect_suspicious_requests(content)
            
            # æ£€æµ‹è¯­è¨€é£æ ¼çªå˜
            style_anomaly = self._detect_style_anomaly(msg, messages)
            
            if attack_indicators or suspicious_requests or style_anomaly:
                threats.append({
                    'message_id': msg.message_id,
                    'timestamp': msg.timestamp,
                    'sender': msg.sender_id,
                    'attack_indicators': attack_indicators,
                    'suspicious_requests': suspicious_requests,
                    'style_anomaly': style_anomaly,
                    'threat_level': self._calculate_threat_level(
                        attack_indicators, suspicious_requests, style_anomaly
                    )
                })
        
        return threats
    
    def _detect_attack_patterns(self, text: str) -> Dict[str, List[str]]:
        """æ£€æµ‹æ”»å‡»æ¨¡å¼"""
        detected = {}
        
        for attack_type, patterns in self.attack_patterns.items():
            matches = []
            for pattern in patterns:
                if re.search(pattern, text):
                    matches.append(pattern)
            if matches:
                detected[attack_type] = matches
        
        return detected
    
    def _detect_suspicious_requests(self, text: str) -> List[str]:
        """æ£€æµ‹å¯ç–‘è¯·æ±‚"""
        detected = []
        for request in self.suspicious_requests:
            if re.search(request, text):
                detected.append(request)
        return detected
    
    def _detect_style_anomaly(self, message: ChatMessage, all_messages: List[ChatMessage]) -> Optional[Dict]:
        """æ£€æµ‹è¯­è¨€é£æ ¼å¼‚å¸¸"""
        # ç®€åŒ–ç‰ˆï¼šæ£€æµ‹æ¶ˆæ¯é•¿åº¦å¼‚å¸¸
        content_length = len(message.content)
        avg_length = np.mean([len(m.content) for m in all_messages])
        std_length = np.std([len(m.content) for m in all_messages])
        
        if std_length > 0 and abs(content_length - avg_length) > 3 * std_length:
            return {
                'type': 'length_anomaly',
                'message_length': content_length,
                'average_length': avg_length,
                'deviation': abs(content_length - avg_length) / std_length
            }
        
        return None
    
    def _calculate_threat_level(
        self, 
        attack_indicators: Dict, 
        suspicious_requests: List[str],
        style_anomaly: Optional[Dict]
    ) -> str:
        """è®¡ç®—å¨èƒç­‰çº§"""
        threat_score = 0
        
        # æ”»å‡»æ¨¡å¼åŠ æƒ
        if 'urgency' in attack_indicators:
            threat_score += 20
        if 'fear' in attack_indicators:
            threat_score += 25
        if 'authority' in attack_indicators:
            threat_score += 20
        if 'trust_exploitation' in attack_indicators:
            threat_score += 30
        
        # å¯ç–‘è¯·æ±‚
        threat_score += len(suspicious_requests) * 15
        
        # é£æ ¼å¼‚å¸¸
        if style_anomaly:
            threat_score += 15
        
        if threat_score >= 60:
            return 'high'
        elif threat_score >= 30:
            return 'medium'
        else:
            return 'low'
    
    def detect_internal_threats(self, messages: List[ChatMessage], threshold: float = 0.7) -> List[Dict]:
        """æ£€æµ‹å†…éƒ¨å¨èƒï¼ˆå‘˜å·¥ç›¸å…³ï¼‰"""
        threats = []
        
        for person_id in set(msg.sender_id for msg in messages):
            person_messages = [m for m in messages if m.sender_id == person_id]
            
            # æƒ…æ„Ÿåˆ†æ
            sentiment_trend = self._analyze_sentiment_trend(person_messages)
            
            # å·¥ä½œæ—¶é—´ç†µåˆ†æ
            time_entropy = self._analyze_time_entropy(person_messages)
            
            # è´Ÿé¢å…³é”®è¯æ£€æµ‹
            negative_keywords = self._detect_negative_keywords(person_messages)
            
            # ç»¼åˆè¯„ä¼°
            internal_risk = self._assess_internal_risk(
                sentiment_trend, time_entropy, negative_keywords
            )
            
            if internal_risk['risk_level'] in ['high', 'critical']:
                threats.append({
                    'person_id': person_id,
                    'sentiment_trend': sentiment_trend,
                    'time_entropy': time_entropy,
                    'negative_keywords': negative_keywords,
                    'risk_assessment': internal_risk
                })
        
        return threats
    
    def _analyze_sentiment_trend(self, messages: List[ChatMessage]) -> Dict:
        """åˆ†ææƒ…æ„Ÿè¶‹åŠ¿"""
        analyzer = SentimentAnalyzer()
        scores = []
        
        for msg in messages:
            result = analyzer.analyze_sentiment(msg.content)
            scores.append(result['scores']['positive'] - result['scores']['negative'])
        
        if len(scores) < 2:
            return {'trend': 'insufficient_data'}
        
        # è®¡ç®—è¶‹åŠ¿
        trend_direction = np.polyfit(range(len(scores)), scores, 1)[0]
        
        return {
            'trend_direction': 'declining' if trend_direction < -0.05 else ('improving' if trend_direction > 0.05 else 'stable'),
            'volatility': np.std(scores),
            'average_score': np.mean(scores)
        }
    
    def _analyze_time_entropy(self, messages: List[ChatMessage]) -> Dict:
        """åˆ†æå·¥ä½œæ—¶é—´ç†µ"""
        hours = [msg.timestamp.hour for msg in messages]
        hist, _ = np.histogram(hours, bins=24, range=(0, 24))
        prob = hist / hist.sum()
        time_entropy = entropy(prob)
        
        return {
            'entropy': time_entropy,
            'interpretation': 'irregular' if time_entropy > 3.5 else ('regular' if time_entropy < 2.5 else 'moderate')
        }
    
    def _detect_negative_keywords(self, messages: List[ChatMessage]) -> List[str]:
        """æ£€æµ‹è´Ÿé¢å…³é”®è¯"""
        negative_words = ['è¾èŒ', 'è·³æ§½', 'ä¸æ»¡', 'è®¨åŒ', 'æ¨', 'ç¦»èŒ', 'æŠ•è¯‰', 'ä¸¾æŠ¥']
        found = []
        
        all_text = ' '.join([m.content for m in messages])
        
        for word in negative_words:
            if word in all_text:
                found.append(word)
        
        return found
    
    def _assess_internal_risk(
        self, 
        sentiment_trend: Dict, 
        time_entropy: Dict,
        negative_keywords: List[str]
    ) -> Dict:
        """è¯„ä¼°å†…éƒ¨é£é™©"""
        risk_score = 0
        
        # æƒ…æ„Ÿè¶‹åŠ¿é£é™©
        if sentiment_trend.get('trend_direction') == 'declining':
            risk_score += 30
        elif sentiment_trend.get('volatility', 0) > 0.5:
            risk_score += 20
        
        # æ—¶é—´ç†µé£é™©
        if time_entropy.get('interpretation') == 'irregular':
            risk_score += 25
        
        # è´Ÿé¢å…³é”®è¯é£é™©
        risk_score += len(negative_keywords) * 15
        
        risk_level = 'critical' if risk_score >= 70 else ('high' if risk_score >= 50 else ('medium' if risk_score >= 30 else 'low'))
        
        return {
            'risk_score': min(100, risk_score),
            'risk_level': risk_level
        }
```

---

## 7. éšç§ä¿æŠ¤ä¸ä¼¦ç†åˆè§„

### 7.1 æ•°æ®è„±æ•æŠ€æœ¯

```python
import hashlib
import re
from cryptography.fernet import Fernet

class DataMasking:
    """æ•°æ®è„±æ•å·¥å…·ç±»"""
    
    @staticmethod
    def mask_phone(phone: str) -> str:
        """æ‰‹æœºå·è„±æ•"""
        if len(phone) >= 11:
            return phone[:3] + '*' * 4 + phone[-4:]
        return phone
    
    @staticmethod
    def mask_email(email: str) -> str:
        """é‚®ç®±è„±æ•"""
        if '@' in email:
            username, domain = email.split('@')
            if len(username) > 2:
                return username[:2] + '*' * (len(username) - 2) + '@' + domain
            elif len(username) > 0:
                return username[0] + '*' + '@' + domain
        return email
    
    @staticmethod
    def mask_name(name: str) -> str:
        """å§“åè„±æ•"""
        if len(name) >= 2:
            return name[0] + '*' * (len(name) - 1)
        return '*' * len(name)
    
    @staticmethod
    def mask_id_card(id_card: str) -> str:
        """èº«ä»½è¯è„±æ•"""
        if len(id_card) == 18:
            return id_card[:6] + '*' * 8 + id_card[-4:]
        return id_card
    
    @staticmethod
    def mask_address(address: str) -> str:
        """åœ°å€è„±æ•ï¼ˆä¿ç•™å‰å‡ ä¸ªå­—ç¬¦ï¼‰"""
        if len(address) > 4:
            return address[:2] + '*' * (len(address) - 4) + address[-2:]
        return '*' * len(address)
    
    @staticmethod
    def hash_identifier(identifier: str) -> str:
        """æ ‡è¯†ç¬¦å“ˆå¸ŒåŒ–"""
        return hashlib.sha256(identifier.encode()).hexdigest()[:16]
    
    @staticmethod
    def mask_bank_card(card_number: str) -> str:
        """é“¶è¡Œå¡å·è„±æ•"""
        if len(card_number) >= 16:
            return card_number[:4] + '*' * (len(card_number) - 8) + card_number[-4:]
        return card_number


class DataEncryption:
    """æ•°æ®åŠ å¯†å·¥å…·ç±»"""
    
    def __init__(self):
        self.key = Fernet.generate_key()
        self.cipher = Fernet(self.key)
    
    def encrypt_data(self, data: str) -> str:
        """åŠ å¯†æ•°æ®"""
        return self.cipher.encrypt(data.encode()).decode()
    
    def decrypt_data(self, encrypted_data: str) -> str:
        """è§£å¯†æ•°æ®"""
        return self.cipher.decrypt(encrypted_data.encode()).decode()
    
    @staticmethod
    def hash_password(password: str) -> str:
        """å¯†ç å“ˆå¸Œ"""
        return hashlib.sha256(password.encode()).hexdigest()
```

### 7.2 åˆè§„æ¡†æ¶

```python
class ComplianceFramework:
    """åˆè§„æ¡†æ¶"""
    
    # æ³•è§„è¦æ±‚
    REGULATIONS = {
        'GDPR': {
            'name': 'é€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹',
            'key_requirements': [
                'æ•°æ®æœ€å°åŒ–',
                'ç›®çš„é™åˆ¶',
                'å­˜å‚¨é™åˆ¶',
                'ç”¨æˆ·åŒæ„',
                'æ•°æ®è®¿é—®æƒ',
                'è¢«é—å¿˜æƒ'
            ]
        },
        'PIPL': {
            'name': 'ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ï¼ˆä¸­å›½ï¼‰',
            'key_requirements': [
                'çŸ¥æƒ…åŒæ„',
                'æœ€å°å¿…è¦',
                'ç›®çš„é™åˆ¶',
                'å®‰å…¨ä¿éšœ',
                'ä¸ªäººæƒåˆ©ä¿éšœ'
            ]
        },
        'CSL': {
            'name': 'æ•°æ®å®‰å…¨æ³•ï¼ˆä¸­å›½ï¼‰',
            'key_requirements': [
                'æ•°æ®åˆ†ç±»åˆ†çº§',
                'æ•°æ®å®‰å…¨ä¿æŠ¤',
                'æ•°æ®è·¨å¢ƒä¼ è¾“ç®¡ç†'
            ]
        }
    }
    
    def __init__(self):
        self.data_retention_policy = 90  # é»˜è®¤ä¿ç•™90å¤©
        self.access_control_policy = {}
    
    def check_compliance(self, data_processing_activity: Dict) -> Dict:
        """æ£€æŸ¥åˆè§„æ€§"""
        issues = []
        
        # æ£€æŸ¥æ•°æ®æœ€å°åŒ–
        if data_processing_activity.get('data_scope') == 'all':
            issues.append({
                'issue': 'å¯èƒ½è¿åæ•°æ®æœ€å°åŒ–åŸåˆ™',
                'recommendation': 'ä»…æ”¶é›†åˆ†ææ‰€éœ€çš„æœ€å°æ•°æ®'
            })
        
        # æ£€æŸ¥ç›®çš„é™åˆ¶
        if not data_processing_activity.get('processing_purpose'):
            issues.append({
                'issue': 'æœªæ˜ç¡®æ•°æ®å¤„ç†ç›®çš„',
                'recommendation': 'åœ¨å¤„ç†å‰æ˜ç¡®å¹¶è®°å½•å¤„ç†ç›®çš„'
            })
        
        # æ£€æŸ¥ç”¨æˆ·åŒæ„
        if not data_processing_activity.get('user_consent'):
            issues.append({
                'issue': 'ç¼ºä¹ç”¨æˆ·æ˜ç¡®åŒæ„',
                'recommendation': 'è·å–å¹¶è®°å½•ç”¨æˆ·æ˜ç¡®åŒæ„'
            })
        
        # æ£€æŸ¥ä¿ç•™æœŸé™
        if not data_processing_activity.get('retention_period'):
            issues.append({
                'issue': 'æœªè®¾ç½®æ•°æ®ä¿ç•™æœŸé™',
                'recommendation': 'è®¾ç½®å¹¶æ‰§è¡Œæ•°æ®ä¿ç•™ç­–ç•¥'
            })
        
        return {
            'compliant': len(issues) == 0,
            'issues': issues,
            'compliance_score': max(0, 100 - len(issues) * 25)
        }
    
    def generate_audit_log(self, activity: str, details: Dict) -> Dict:
        """ç”Ÿæˆå®¡è®¡æ—¥å¿—"""
        return {
            'timestamp': datetime.now().isoformat(),
            'activity': activity,
            'details': details,
            'user_id': details.get('operator_id', 'anonymous'),
            'ip_address': details.get('ip_address', 'unknown')
        }
```

### 7.3 ä¼¦ç†å‡†åˆ™

```python
class EthicalGuidelines:
    """ä¼¦ç†å‡†åˆ™"""
    
    @staticmethod
    def transparency_principle() -> Dict:
        """é€æ˜æ€§åŸåˆ™"""
        return {
            'description': 'ç¡®ä¿åˆ†æè¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§',
            'requirements': [
                'å‘ç”¨æˆ·è¯´æ˜æ•°æ®æ”¶é›†å’Œåˆ†æçš„èŒƒå›´',
                'æä¾›ç®—æ³•å†³ç­–çš„å¯è§£é‡Šè¯´æ˜',
                'å…¬å¼€ä½¿ç”¨çš„æ•°æ®ç±»å‹å’Œåˆ†ææ–¹æ³•'
            ]
        }
    
    @staticmethod
    def consent_principle() -> Dict:
        """åŒæ„åŸåˆ™"""
        return {
            'description': 'è·å–ç”¨æˆ·çš„æ˜ç¡®åŒæ„',
            'requirements': [
                'åœ¨æ”¶é›†æ•°æ®å‰è·å–çŸ¥æƒ…åŒæ„',
                'å…è®¸ç”¨æˆ·é€‰æ‹©é€€å‡ºåˆ†æ',
                'æ˜ç¡®å‘ŠçŸ¥æ•°æ®ä½¿ç”¨ç›®çš„'
            ]
        }
    
    @staticmethod
    def harm_prevention_principle() -> Dict:
        """ä¼¤å®³é¢„é˜²åŸåˆ™"""
        return {
            'description': 'é˜²æ­¢åˆ†æç»“æœè¢«ç”¨äºä¼¤å®³ç›®çš„',
            'requirements': [
                'ç¦æ­¢ç”¨äºæ­§è§†æ€§ç›®çš„',
                'ç¦æ­¢ç”¨äºæœªç»æˆæƒçš„ç›‘æ§',
                'ç¦æ­¢ç”¨äºæ•²è¯ˆæˆ–èƒè¿«'
            ]
        }
    
    @staticmethod
    def bias_mitigation_principle() -> Dict:
        """åè§ç¼“è§£åŸåˆ™"""
        return {
            'description': 'è¯†åˆ«å’Œçº æ­£ç®—æ³•åè§',
            'requirements': [
                'å®šæœŸæµ‹è¯•ç®—æ³•åè§',
                'ä½¿ç”¨å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®',
                'ç›‘æ§åè§æŒ‡æ ‡'
            ]
        }
    
    @staticmethod
    def accountability_principle() -> Dict:
        """é—®è´£åŸåˆ™"""
        return {
            'description': 'å»ºç«‹æ¸…æ™°çš„è´£ä»»æœºåˆ¶',
            'requirements': [
                'æ˜ç¡®æ•°æ®æ§åˆ¶è€…è´£ä»»',
                'å»ºç«‹æŠ•è¯‰å’Œç”³è¯‰æœºåˆ¶',
                'å®šæœŸè¿›è¡Œåˆè§„å®¡è®¡'
            ]
        }
```

---

## 8. å·¥å…·ä¸æŠ€æœ¯æ ˆ

### 8.1 Pythonä¾èµ–åº“

```txt
# requirements.txt

# åŸºç¡€ä¾èµ–
pandas>=2.0.0
numpy>=1.24.0
scipy>=1.10.0

# NLPä¾èµ–
jieba>=0.42.1
pkuseg>=0.0.28  # åŒ—å¤§æ¦‚ç‡è¯æ€§æ ‡æ³¨å·¥å…·
transformers>=4.30.0
torch>=2.0.0
torchvision>=0.15.0
sentence-transformers>=0.4.1
scikit-learn>=1.3.0

# æ–‡æœ¬å¤„ç†
regex>=2023.0
unidecode>=1.3.6

# æ•°æ®å­˜å‚¨
sqlite3  # æ ‡å‡†åº“
redis>=4.5.0
elasticsearch>=8.0.0

# å¯è§†åŒ–
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0
pyecharts>=2.0.0

# Webæ¡†æ¶ï¼ˆå¯é€‰ï¼‰
fastapi>=0.100.0
uvicorn>=0.22.0
flask>=2.3.0

# åŠ å¯†ä¸å®‰å…¨
cryptography>=41.0.0

# æ—¥å¿—
loguru>=0.7.0

# é…ç½®ç®¡ç†
pyyaml>=6.0
python-dotenv>=1.0.0

# è¿›åº¦æ¡
tqdm>=4.65.0

# å¼‚æ­¥æ”¯æŒ
asyncio>=3.4.3
aiohttp>=3.8.0
```

### 8.2 å¸¸ç”¨å·¥å…·å¯¹æ¯”

| ç±»åˆ« | å·¥å…· | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | å±€é™ |
|-----|------|---------|------|------|
| **åˆ†è¯** | jieba | é€šç”¨ä¸­æ–‡åˆ†è¯ | å¼€æºã€è¯åº“ä¸°å¯Œ | ä¸“ä¸šé¢†åŸŸæ•ˆæœä¸€èˆ¬ |
| | pkuseg | å­¦æœ¯/ä¸“ä¸šé¢†åŸŸ | å‡†ç¡®ç‡é«˜ | éœ€è¦æ ‡æ³¨æ•°æ® |
| | HanLP | å…¨é¢NLPä»»åŠ¡ | åŠŸèƒ½å…¨é¢ | èµ„æºæ¶ˆè€—å¤§ |
| **æƒ…æ„Ÿåˆ†æ** | SnowNLP | å¿«é€Ÿä½“éªŒ | ç®€å•æ˜“ç”¨ | å‡†ç¡®ç‡ä¸€èˆ¬ |
| | TextBlob | è‹±æ–‡ä¸ºä¸» | å¤šè¯­è¨€æ”¯æŒ | ä¸­æ–‡æ•ˆæœæœ‰é™ |
| | BERT-based | é«˜ç²¾åº¦åœºæ™¯ | å‡†ç¡®ç‡é«˜ | èµ„æºæ¶ˆè€—å¤§ |
| **ä¸»é¢˜å»ºæ¨¡** | LDA | ä¼ ç»Ÿä¸»é¢˜å»ºæ¨¡ | æˆç†Ÿç¨³å®š | éš¾ä»¥å¤„ç†çŸ­æ–‡æœ¬ |
| | BERTopic | è¯­ä¹‰ä¸»é¢˜å»ºæ¨¡ | è¯­ä¹‰ç†è§£å¼º | ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹ |
| **å¯è§†åŒ–** | Gephi | ç½‘ç»œå¯è§†åŒ– | ä¸“ä¸šå¼ºå¤§ | å­¦ä¹ æ›²çº¿é™¡ |
| | PyECharts | äº¤äº’å¼å›¾è¡¨ | ç®€å•æ˜“ç”¨ | å®šåˆ¶æ€§æœ‰é™ |
| | Matplotlib | åŸºç¡€å›¾è¡¨ | é«˜åº¦å¯æ§ | æ ·å¼è¿‡æ—¶ |

### 8.3 2024-2025å¹´æœ€æ–°æŠ€æœ¯

æ ¹æ®æœ€æ–°ç ”ç©¶åŠ¨æ€ï¼Œä»¥ä¸‹æŠ€æœ¯å€¼å¾—å…³æ³¨ï¼š

1. **LLMé©±åŠ¨çš„æ—¥å¿—åˆ†æ**
   - LLMLogAnalyzerï¼šåŸºäºLLMçš„æ—¥å¿—åˆ†æèŠå¤©æœºå™¨äºº
   - LogRulesï¼šé€šè¿‡è§„åˆ™å¢å¼ºLLMçš„æ—¥å¿—åˆ†æèƒ½åŠ›

2. **èšåˆå¼é—®ç­”**
   - Aggregative Question Answeringï¼šè·¨å¤§è§„æ¨¡å¯¹è¯æ—¥å¿—è¿›è¡Œæ¨ç†

3. **å®æ—¶å¯è§†åŒ–**
   - WildVisï¼šç™¾ä¸‡çº§èŠå¤©æ—¥å¿—å¼€æºå¯è§†åŒ–å™¨

4. **å¤šæ¨¡æ€åˆ†æ**
   - å›¾åƒ+æ–‡æœ¬è”åˆåˆ†æ
   - è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«

5. **å¤§æ¨¡å‹äººæ ¼æ£€æµ‹**
   - åŸºäºLLMçš„äººæ ¼æ¨æ–­
   - å¤šæ¨¡æ€äººæ ¼ç‰¹å¾èåˆ

---

## 9. å®æ–½è·¯çº¿å›¾

### 9.1 åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

```
Phase 1: åŸºç¡€è®¾æ–½æ­å»º (4-6å‘¨)
â”œâ”€â”€ å®Œæˆå¼€å‘ç¯å¢ƒé…ç½®
â”œâ”€â”€ å»ºç«‹æ•°æ®é‡‡é›†ç®¡é“
â”œâ”€â”€ å®ç°æ•°æ®é¢„å¤„ç†æ¨¡å—
â””â”€â”€ æ­å»ºåŸºç¡€å­˜å‚¨æ¶æ„

Phase 2: æ ¸å¿ƒåˆ†æåŠŸèƒ½ (6-8å‘¨)
â”œâ”€â”€ å®ç°æ—¶é—´åˆ†ææ¨¡å—
â”œâ”€â”€ å®ç°å†…å®¹åˆ†ææ¨¡å—
â”œâ”€â”€ å®ç°æƒ…æ„Ÿåˆ†ææ¨¡å—
â””â”€â”€ å®ç°ç¤¾äº¤åˆ†ææ¨¡å—

Phase 3: é«˜çº§åˆ†æåŠŸèƒ½ (6-8å‘¨)
â”œâ”€â”€ å®ç°æ€§æ ¼åˆ†ææ¨¡å—
â”œâ”€â”€ å®ç°ä¸»é¢˜å»ºæ¨¡æ¨¡å—
â”œâ”€â”€ å®ç°å¨èƒæ£€æµ‹æ¨¡å—
â””â”€â”€ å®ç°è¡Œä¸ºé¢„æµ‹æ¨¡å—

Phase 4: ç³»ç»Ÿé›†æˆ (4-6å‘¨)
â”œâ”€â”€ APIæœåŠ¡å¼€å‘
â”œâ”€â”€ å¯è§†åŒ–ç•Œé¢å¼€å‘
â”œâ”€â”€ æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ
â””â”€â”€ å‘Šè­¦æ¨é€ç³»ç»Ÿ

Phase 5: ä¼˜åŒ–ä¸æ‰©å±• (æŒç»­)
â”œâ”€â”€ æ€§èƒ½ä¼˜åŒ–
â”œâ”€â”€ æ–°åŠŸèƒ½å¼€å‘
â”œâ”€â”€ æ¨¡å‹è¿­ä»£æ›´æ–°
â””â”€â”€ åˆè§„æ€§å®¡è®¡
```

### 9.2 å…³é”®é‡Œç¨‹ç¢‘

| é˜¶æ®µ | é‡Œç¨‹ç¢‘ | äº¤ä»˜ç‰© |
|-----|-------|-------|
| Phase 1 | ç¯å¢ƒå°±ç»ª | æŠ€æœ¯æ¶æ„æ–‡æ¡£ã€æ•°æ®ç®¡é“ |
| Phase 2 | æ ¸å¿ƒåŠŸèƒ½å®Œæˆ | åˆ†æå¼•æ“ã€APIæ¥å£ |
| Phase 3 | é«˜çº§åŠŸèƒ½å®Œæˆ | æ€§æ ¼æŠ¥å‘Šã€å¨èƒå‘Šè­¦ |
| Phase 4 | ç³»ç»Ÿé›†æˆå®Œæˆ | å¯è§†åŒ–å¹³å°ã€å®Œæ•´æŠ¥å‘Š |
| Phase 5 | ç”Ÿäº§å°±ç»ª | æ€§èƒ½æµ‹è¯•æŠ¥å‘Šã€å®‰å…¨å®¡è®¡ |

### 9.3 è´¨é‡ä¿è¯

```python
class QualityAssurance:
    """è´¨é‡ä¿è¯æ£€æŸ¥"""
    
    @staticmethod
    def validate_data_quality(messages: List[ChatMessage]) -> Dict:
        """éªŒè¯æ•°æ®è´¨é‡"""
        validation_results = {
            'total_messages': len(messages),
            'missing_fields': [],
            'duplicate_messages': 0,
            'outlier_detection': [],
            'completeness_score': 0.0
        }
        
        # æ£€æŸ¥ç¼ºå¤±å­—æ®µ
        required_fields = ['message_id', 'timestamp', 'sender_id', 'content']
        for msg in messages:
            for field in required_fields:
                if not getattr(msg, field, None):
                    validation_results['missing_fields'].append({
                        'message_id': msg.message_id,
                        'field': field
                    })
        
        # æ£€æµ‹é‡å¤æ¶ˆæ¯
        seen_ids = set()
        for msg in messages:
            if msg.message_id in seen_ids:
                validation_results['duplicate_messages'] += 1
            else:
                seen_ids.add(msg.message_id)
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        lengths = [len(msg.content) for msg in messages]
        mean_len = np.mean(lengths)
        std_len = np.std(lengths)
        
        for i, msg in enumerate(messages):
            if abs(len(msg.content) - mean_len) > 3 * std_len:
                validation_results['outlier_detection'].append({
                    'message_id': msg.message_id,
                    'length': len(msg.content),
                    'z_score': (len(msg.content) - mean_len) / std_len
                })
        
        # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
        validation_results['completeness_score'] = max(0, 100 - (
            len(validation_results['missing_fields']) * 5 +
            validation_results['duplicate_messages'] * 2 +
            len(validation_results['outlier_detection']) * 3
        ))
        
        return validation_results
    
    @staticmethod
    def validate_analysis_results(results: Dict) -> Dict:
        """éªŒè¯åˆ†æç»“æœ"""
        validation = {
            'has_time_analysis': 'time_analysis' in results,
            'has_content_analysis': 'content_analysis' in results,
            'has_sentiment_analysis': 'sentiment_analysis' in results,
            'has_social_analysis': 'social_analysis' in results,
            'has_personality_analysis': 'personality_analysis' in results,
            'confidence_score': 0.0
        }
        
        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        passed_checks = sum([
            validation['has_time_analysis'],
            validation['has_content_analysis'],
            validation['has_sentiment_analysis'],
            validation['has_social_analysis'],
            validation['has_personality_analysis']
        ])
        
        validation['confidence_score'] = passed_checks / 5 * 100
        
        return validation
```

---

## 10. é™„å½•

### 10.1 æœ¯è¯­è¡¨

| æœ¯è¯­ | å®šä¹‰ |
|-----|------|
| **TF-IDF** | è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼Œç”¨äºè¯„ä¼°è¯çš„é‡è¦ç¨‹åº¦ |
| **TextRank** | åŸºäºå›¾çš„å…³é”®å­—æå–ç®—æ³• |
| **LDA** | æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…ï¼Œä¸»é¢˜å»ºæ¨¡æ–¹æ³• |
| **LIWC** | è¯­è¨€è¯¢é—®ä¸å­—æ•°ç»Ÿè®¡ï¼Œå¿ƒç†è¯­è¨€å­¦å·¥å…· |
| **SNA** | ç¤¾ä¼šç½‘ç»œåˆ†æ |
| **PII** | ä¸ªäººèº«ä»½ä¿¡æ¯ |
| **Entropy** | é¦™å†œç†µï¼Œè¡¡é‡ä¿¡æ¯ä¸ç¡®å®šæ€§ |
| **BERT** | åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ |

### 10.2 å‚è€ƒæ–‡çŒ®

1. Mining Individual Life Pattern Based on Location History - Columbia University
2. Echoes of Power: Language Effects and Power Differences in Social Interaction - Cornell
3. SIFRank: Chinese Keyword Extraction Using Pretrained Language Model - NLPIR
4. From Precision to Perception: User-Centred Evaluation of Keyword Extraction - arXiv
5. Sentiment Analysis in the Era of Large Language Models - NAACL 2024
6. Personality Expression in Chinese Language Use - ResearchGate
7. WildChat: 1M ChatGPT Interaction Logs in the Wild - arXiv 2024
8. LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot - arXiv 2024

### 10.3 æ‰©å±•é˜…è¯»èµ„æº

| èµ„æºç±»å‹ | é“¾æ¥/æ¥æº |
|---------|---------|
| **arXivè®ºæ–‡** | arxiv.org (æœç´¢chat log analysis, sentiment analysis, personality detection) |
| **ä¸­æ–‡NLP** | github.com/fxsjy/jieba |
| **æƒ…æ„Ÿåˆ†æ** | github.com/baidu/LAC |
| **ä¸»é¢˜å»ºæ¨¡** | github.com/MaartenGr/BERTopic |
| **å¯è§†åŒ–** | gephi.org |

---

**é‡è¦å£°æ˜**

æœ¬æ–¹æ¡ˆä»…ä¾›ç ”ç©¶å’Œå­¦ä¹ ç›®çš„ä½¿ç”¨ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¯·åŠ¡å¿…ï¼š

1. éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„
2. è·å–å¿…è¦çš„ç”¨æˆ·æˆæƒ
3. ä¿æŠ¤ä¸ªäººéšç§å’Œæ•°æ®å®‰å…¨
4. é¿å…å°†æŠ€æœ¯ç”¨äºéæ³•æˆ–ä¸é“å¾·ç›®çš„

---

**æ–‡æ¡£ç‰ˆæœ¬å†å²**

| ç‰ˆæœ¬ | æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|-----|------|---------|
| 1.0 | 2025-02-05 | åˆå§‹ç‰ˆæœ¬ï¼Œæ•´åˆä¸‰ä»½æ–‡æ¡£å¹¶è¡¥å……æœ€æ–°æŠ€æœ¯ |
